{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6232a3ec",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size: 28px;\"><b>NaiveBayes Classifier</b></p>\n",
    "\n",
    "## Notebook Overview — Bayes Refresher & Naive Bayes (Text)**\n",
    "\n",
    "This notebook moves from Bayes’ rule to a working Naive Bayes sentiment classifier for short product reviews.\n",
    "\n",
    "### Bayes refresher\n",
    "We compare posterior probabilities of two classes \\(c \\in \\{\\text{positive}, \\text{negative}\\}\\) for a given review:\n",
    "\\[\n",
    "P(c \\mid \\text{review}) \\;=\\; \\frac{P(\\text{review}\\mid c)\\,P(c)}{P(\\text{review})}.\n",
    "\\]\n",
    "For deciding between classes, the common denominator cancels:\n",
    "\\[\n",
    "\\arg\\max_{c} \\; P(\\text{review}\\mid c)\\,P(c).\n",
    "\\]\n",
    "\n",
    "### Naive Bayes classifier (theory)\n",
    "- Naive assumption (conditional independence): given the class \\(c\\), words in a review are independent.\n",
    "- Bag-of-words likelihood:\n",
    "\\[\n",
    "P(\\text{review}\\mid c) \\;=\\; \\prod_{w \\in \\text{review}} P(w \\mid c).\n",
    "\\]\n",
    "- Laplace smoothing (avoid zeros for unseen words):\n",
    "\\[\n",
    "P(w \\mid c) \\;=\\; \\frac{\\#(w \\text{ in class } c)+1}{\\text{total words in } c + N},\n",
    "\\]\n",
    "where \\(N\\) is the number of unique tokens in the vocabulary.\n",
    "- Numerical stability: compare in log-space\n",
    "\\[\n",
    "\\log P(\\text{review}\\mid c) + \\log P(c) \\;=\\; \\sum_{w}\\log P(w\\mid c) + \\log P(c).\n",
    "\\]\n",
    "\n",
    "### From theory to practice\n",
    "- Text \\(\\rightarrow\\) features via CountVectorizer (bag-of-words counts).\n",
    "- Train MultinomialNB with \\(X=\\) counts and \\(y=\\) labels.\n",
    "- Predict with `.predict()`; inspect confidence with `.predict_proba()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d4c6c2",
   "metadata": {},
   "source": [
    "# Bayes Theorem refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093de33e",
   "metadata": {},
   "source": [
    "## Independent Events\n",
    "\n",
    "Two events are **independent** when the occurrence of one **does not influence** the probability of the other. In such cases, knowing that one event has occurred gives **no information** about whether the other will occur.\n",
    "\n",
    "**Examples of independent events:**\n",
    "- I wear a blue shirt; my coworker wears a blue shirt.  \n",
    "- I take the subway to work; I eat sushi for lunch.  \n",
    "- The NY Giants win their football game; the NY Rangers win their hockey game.  \n",
    "\n",
    "Mathematically, independence between two events \\( A \\) and \\( B \\) is expressed as:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\times P(B)\n",
    "$$\n",
    "\n",
    "If this equality does **not** hold, the events are **dependent** — meaning the occurrence of one event affects the probability of the other.\n",
    "\n",
    "**Examples of dependent events:**\n",
    "- It rains on Tuesday; I carry an umbrella on Tuesday.  \n",
    "- I eat spaghetti; I have a red stain on my shirt.  \n",
    "- I wear sunglasses; I go to the beach.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5b322",
   "metadata": {},
   "source": [
    "## Conditional Probability\n",
    "\n",
    "**Conditional probability** describes the likelihood of two events happening together. It is most straightforward to calculate when the events are **independent**.\n",
    "\n",
    "In probability notation, we denote the probability of an event as \\( P(\\text{event}) \\).\n",
    "\n",
    "If the probability of event \\( A \\) is \\( P(A) \\) and the probability of event \\( B \\) is \\( P(B) \\), and the two events are **independent**, then the probability that both occur is given by:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\times P(B)\n",
    "$$\n",
    "\n",
    "Here, the symbol \\( \\cap \\) means “and”, so \\( P(A \\cap B) \\) represents the probability that **both** \\( A \\) and \\( B \\) happen.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Rolling Two Sixes\n",
    "\n",
    "Suppose we roll a pair of dice and want to know the probability of getting two sixes.\n",
    "\n",
    "Each die has six sides, so the probability of rolling a six is \\( \\frac{1}{6} \\).  \n",
    "Because the dice rolls are **independent** (rolling one six does not affect the other), the joint probability is:\n",
    "\n",
    "$$\n",
    "P(6 \\cap 6) = P(6) \\times P(6) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36}\n",
    "$$\n",
    "\n",
    "Thus, the probability of rolling two sixes is **1/36**.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81bbd5",
   "metadata": {},
   "source": [
    "## Conditional Probability and Independence\n",
    "\n",
    "### 1. General Definition of Conditional Probability\n",
    "\n",
    "The **conditional probability** of event \\( A \\) given event \\( B \\) is defined as:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "This means:\n",
    "\n",
    "> The probability that \\( A \\) occurs **given that** \\( B \\) has already occurred equals the probability that both events happen divided by the probability that \\( B \\) happens.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Independence — What It Means\n",
    "\n",
    "Two events \\( A \\) and \\( B \\) are **independent** if knowing that one occurred gives **no information** about the other.\n",
    "\n",
    "Mathematically, this is expressed as:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = P(A)\n",
    "$$\n",
    "\n",
    "If the occurrence of \\( B \\) does not change the probability of \\( A \\), then \\( A \\) and \\( B \\) are independent.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Combining the Two Equations\n",
    "\n",
    "From the **definition of conditional probability**:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "and from **independence**:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = P(A)\n",
    "$$\n",
    "\n",
    "Setting these equal gives:\n",
    "\n",
    "$$\n",
    "P(A) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Multiplying both sides by \\( P(B) \\):\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A) \\times P(B)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Interpretation\n",
    "\n",
    "For **independent events**, the probability that both occur equals the **product of their individual probabilities**, because independence means the occurrence of one event has **no effect** on the likelihood of the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1df223da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of both raining and going to the gym is: 0.18\n"
     ]
    }
   ],
   "source": [
    "# ## Conditional Probability and Independence\n",
    "#\n",
    "# This code demonstrates the relationship between two independent events using probability.\n",
    "#\n",
    "# Step 1: Define the concept\n",
    "# If two events A and B are independent, the probability that both occur equals\n",
    "# the product of their individual probabilities:\n",
    "#\n",
    "#     P(A ∩ B) = P(A) × P(B)\n",
    "#\n",
    "# Step 2: Define the probabilities of the two events\n",
    "# Let's assume:\n",
    "# - Event A: Probability of rain = 3/5\n",
    "# - Event B: Probability of going to the gym = 0.3\n",
    "#\n",
    "# Step 3: Since they are independent, compute the probability of both events happening\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Probability of A (rain)\n",
    "p_rain = 3 / 5\n",
    "\n",
    "# Probability of B (going to the gym)\n",
    "p_gym = 0.3\n",
    "\n",
    "# Joint probability for independent events\n",
    "p_rain_and_gym = p_rain * p_gym\n",
    "\n",
    "# Step 4: Print the result with an explanatory message\n",
    "print(f\"The probability of both raining and going to the gym is: {p_rain_and_gym:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4e371",
   "metadata": {},
   "source": [
    "## Testing for a Rare Disease\n",
    "\n",
    "Imagine you are a doctor testing a patient for a **rare disease**. The test is highly accurate — it gives the correct result **99% of the time**. However, the disease itself is extremely rare, occurring in only **1 out of 100,000** patients.\n",
    "\n",
    "You run the test and the result is **positive**. At first glance, it may seem that the patient almost certainly has the disease, since the test is only wrong 1% of the time.  \n",
    "But this intuition is misleading — because the **rarity of the disease** drastically affects the true probability.\n",
    "\n",
    "When the test result is positive, there are two possible scenarios:\n",
    "\n",
    "1. The patient **has** the disease, and the test correctly identifies it.  \n",
    "2. The patient **does not have** the disease, but the test incorrectly reports a positive result (a **false positive**).\n",
    "\n",
    "This problem illustrates the importance of considering **base rates** (the overall rarity of the condition) when interpreting test results — a concept at the heart of **Bayesian reasoning**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7141cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability (disease and correctly diagnosed): 0.00000990 (0.000990%)\n",
      "Probability (no disease and incorrectly diagnosed): 0.00999990 (0.999990%)\n"
     ]
    }
   ],
   "source": [
    "# ## Testing for a Rare Disease\n",
    "#\n",
    "# Step 1: Define the known probabilities\n",
    "# - Probability of being sick (disease prevalence): 1 in 100,000\n",
    "# - Probability of being healthy: 1 - sick_rate\n",
    "# - Probability that the test is correct: 99%\n",
    "# - Probability that the test is incorrect (false result): 1%\n",
    "#\n",
    "# Step 2: Calculate the probabilities of two key scenarios:\n",
    "#   a) The patient has the disease AND the test correctly identifies it.\n",
    "#   b) The patient does not have the disease AND the test incorrectly identifies it.\n",
    "#\n",
    "# Step 3: Save the results into variables and print them as both raw values and percentages.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define base probabilities\n",
    "sick_rate = 1 / 100000           # Probability of having the disease\n",
    "health_rate = 1 - sick_rate      # Probability of not having the disease\n",
    "test_correct = 0.99              # Test correctly identifies the condition\n",
    "test_false = 1 - test_correct    # Test gives a false result\n",
    "\n",
    "# Probability of having the disease and the test being correct\n",
    "p_disease_and_correct = sick_rate * test_correct\n",
    "\n",
    "# Probability of not having the disease and the test being incorrect\n",
    "p_no_disease_and_incorrect = health_rate * test_false\n",
    "\n",
    "# Print results\n",
    "print(f\"Probability (disease and correctly diagnosed): {p_disease_and_correct:.8f} ({p_disease_and_correct * 100:.6f}%)\")\n",
    "print(f\"Probability (no disease and incorrectly diagnosed): {p_no_disease_and_incorrect:.8f} ({p_no_disease_and_incorrect * 100:.6f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26c5c5",
   "metadata": {},
   "source": [
    "## Bayes’ Theorem\n",
    "\n",
    "In the previous example, we found two key probabilities:\n",
    "\n",
    "- The patient **had the disease**, and the test correctly diagnosed it: ≈ 0.00001  \n",
    "- The patient **did not have the disease**, but the test incorrectly diagnosed it: ≈ 0.01  \n",
    "\n",
    "Although both events are rare, the false positive was about **1,000 times more likely** than a true positive.  \n",
    "This illustrates the importance of considering **disease prevalence** — not just test accuracy — when interpreting diagnostic results.\n",
    "\n",
    "---\n",
    "\n",
    "### Conditional Probability and Bayes’ Theorem\n",
    "\n",
    "In statistics, we represent the probability of event \\( A \\) occurring **given that** event \\( B \\) has occurred as \\( P(A \\mid B) \\).\n",
    "\n",
    "In our medical test scenario, we want to find:\n",
    "\n",
    "$$\n",
    "P(\\text{rare disease} \\mid \\text{positive result})\n",
    "$$\n",
    "\n",
    "That is, the probability that the patient **has the disease given** that the test result was **positive**.\n",
    "\n",
    "Bayes’ Theorem provides a way to calculate this:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- \\( P(A) \\) = prior probability of the event (e.g., having the disease)  \n",
    "- \\( P(B \\mid A) \\) = probability of a positive result **given** the disease is present (test sensitivity)  \n",
    "- \\( P(B) \\) = overall probability of a positive result (true positives + false positives)\n",
    "\n",
    "---\n",
    "\n",
    "### Applying It to the Example\n",
    "\n",
    "In this context:\n",
    "\n",
    "$$\n",
    "P(\\text{rare disease} \\mid \\text{positive result}) = \\frac{P(\\text{positive result} \\mid \\text{rare disease}) \\cdot P(\\text{rare disease})}{P(\\text{positive result})}\n",
    "$$\n",
    "\n",
    "This formula helps us **update our belief** about how likely it is that the patient truly has the disease **after seeing the test result**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The terms \\( P(A \\mid B) \\) and \\( P(B \\mid A) \\) are **not the same** — the order matters:\n",
    "\n",
    "- \\( P(A \\mid B) \\): probability the patient **has** the disease given the test was positive  \n",
    "- \\( P(B \\mid A) \\): probability the test is **positive given** the patient has the disease  \n",
    "\n",
    "Bayes’ Theorem allows us to correctly relate these two conditional probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b141346b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity (P(positive | disease)): 99.00%\n",
      "Specificity (P(negative | no disease)): 99.00%\n",
      "False Positive Rate (P(positive | no disease)): 1.00%\n",
      "\n",
      "Disease prevalence (P(disease)): 0.00100%\n",
      "No disease (P(no disease)): 99.99900%\n",
      "\n",
      "Overall probability of a positive test (P(positive)): 1.0010%\n",
      "Probability of disease given a positive test (P(disease | positive)): 0.0989%\n",
      "\n",
      "Interpretation:\n",
      "Even though the test is 99% sensitive and 99% specific, the disease is so rare that a positive test result only implies about 0.0989% chance of actually having the disease (roughly 1 in 1,011).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the test characteristics\n",
    "# Sensitivity measures how well the test identifies true positives.\n",
    "# Specificity measures how well the test identifies true negatives.\n",
    "# Here, the test is said to be \"99% accurate,\" which we interpret as:\n",
    "#   - 99% sensitivity (correctly detects sick people)\n",
    "#   - 99% specificity (correctly clears healthy people)\n",
    "# In this case, the *values* are equal (both 99%), but conceptually, they refer to\n",
    "# different directions of correctness.\n",
    "\n",
    "p_positive_given_disease = 0.99     # Sensitivity\n",
    "p_negative_given_no_disease = 0.99  # Specificity\n",
    "p_positive_given_no_disease = 1 - p_negative_given_no_disease  # False positive rate\n",
    "\n",
    "print(f\"Sensitivity (P(positive | disease)): {p_positive_given_disease*100:.2f}%\")\n",
    "print(f\"Specificity (P(negative | no disease)): {p_negative_given_no_disease*100:.2f}%\")\n",
    "print(f\"False Positive Rate (P(positive | no disease)): {p_positive_given_no_disease*100:.2f}%\")\n",
    "\n",
    "# Step 2: Define disease prevalence (extremely rare)\n",
    "p_disease = 1 / 100000\n",
    "p_no_disease = 1 - p_disease\n",
    "\n",
    "print(f\"\\nDisease prevalence (P(disease)): {p_disease*100:.5f}%\")\n",
    "print(f\"No disease (P(no disease)): {p_no_disease*100:.5f}%\")\n",
    "\n",
    "# Step 3: Compute the total probability of testing positive (Law of Total Probability)\n",
    "# This combines true positives and false positives.\n",
    "p_positive = (p_disease * p_positive_given_disease) + (p_no_disease * p_positive_given_no_disease)\n",
    "print(f\"\\nOverall probability of a positive test (P(positive)): {p_positive*100:.4f}%\")\n",
    "\n",
    "# Step 4: Apply Bayes’ Theorem\n",
    "# P(disease | positive) = [P(positive | disease) * P(disease)] / P(positive)\n",
    "p_disease_given_positive = (p_positive_given_disease * p_disease) / p_positive\n",
    "print(f\"Probability of disease given a positive test (P(disease | positive)): {p_disease_given_positive*100:.4f}%\")\n",
    "\n",
    "# Step 5: Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"Even though the test is 99% sensitive and 99% specific, the disease is so rare \"\n",
    "      f\"that a positive test result only implies about {p_disease_given_positive*100:.4f}% \"\n",
    "      f\"chance of actually having the disease (roughly 1 in {int(1/p_disease_given_positive):,}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e58e09",
   "metadata": {},
   "source": [
    "## Spam Filters and Bayes’ Theorem\n",
    "\n",
    "Email **spam filters** apply Bayes’ Theorem to estimate how likely a message is spam, given the words it contains.  \n",
    "For example, consider the word **“enhancement”**, which tends to appear more frequently in spam than in regular emails.\n",
    "\n",
    "We know the following facts:\n",
    "\n",
    "- “enhancement” appears in **0.1% of non-spam emails**\n",
    "- “enhancement” appears in **5% of spam emails**\n",
    "- Spam emails make up **20% of all emails**\n",
    "\n",
    "We want to find:\n",
    "\n",
    "\\[\n",
    "P(\\text{spam} \\mid \\text{enhancement})\n",
    "\\]\n",
    "— the probability that an email is **spam**, given that it contains the word “enhancement”.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Define the probabilities\n",
    "\n",
    "\\[\n",
    "P(\\text{enhancement} \\mid \\text{spam}) = 0.05\n",
    "\\]  \n",
    "\\[\n",
    "P(\\text{enhancement} \\mid \\text{not spam}) = 0.001\n",
    "\\]  \n",
    "\\[\n",
    "P(\\text{spam}) = 0.2\n",
    "\\]  \n",
    "\\[\n",
    "P(\\text{not spam}) = 0.8\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Find the total probability that an email contains “enhancement”\n",
    "\n",
    "By the **law of total probability**:\n",
    "\n",
    "\\[\n",
    "P(\\text{enhancement}) =\n",
    "P(\\text{enhancement} \\mid \\text{spam}) P(\\text{spam}) +\n",
    "P(\\text{enhancement} \\mid \\text{not spam}) P(\\text{not spam})\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= (0.05 \\times 0.2) + (0.001 \\times 0.8)\n",
    "= 0.0108\n",
    "\\]\n",
    "\n",
    "So, about **1.08% of all emails** contain the word “enhancement.”\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Apply Bayes’ Theorem\n",
    "\n",
    "\\[\n",
    "P(\\text{spam} \\mid \\text{enhancement}) =\n",
    "\\frac{P(\\text{enhancement} \\mid \\text{spam}) \\times P(\\text{spam})}{P(\\text{enhancement})}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= \\frac{0.05 \\times 0.2}{0.0108}\n",
    "= 0.9259\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Interpretation\n",
    "\n",
    "Even though spam is only **20% of all emails**, the presence of “enhancement” makes it **92.6% likely** that the email is spam.  \n",
    "This is the essence of **Bayesian filtering** — updating our belief based on evidence.\n",
    "\n",
    "A spam filter uses this principle for many words simultaneously, continuously learning which ones best predict spam-like behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf4fbeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam) = 20.0%\n",
      "P(enhancement | spam) = 5.0%\n",
      "P(enhancement | not spam) = 0.10%\n",
      "P(enhancement) = 1.08%\n",
      "\n",
      "P(spam | enhancement) = 92.59%\n",
      "\n",
      "Interpretation:\n",
      "If an email contains 'enhancement', there is a 92.59% chance it is spam.\n",
      "However, about 0.10% of legitimate emails also contain 'enhancement',\n",
      "so a spam filter should not rely on a single word but combine multiple indicators for accuracy.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the events\n",
    "# A = \"spam\"\n",
    "# B = \"enhancement\"\n",
    "# We want to find: P(spam | enhancement)\n",
    "\n",
    "a = 'spam'\n",
    "b = 'enhancement'\n",
    "\n",
    "# Step 2: Base probabilities\n",
    "# 20% of emails are spam\n",
    "p_spam = 0.2\n",
    "\n",
    "# 80% are not spam\n",
    "p_no_spam = 1 - p_spam\n",
    "\n",
    "# \"enhancement\" appears in 5% of spam emails\n",
    "p_enhancement_given_spam = 0.05\n",
    "\n",
    "# \"enhancement\" appears in 0.1% of non-spam emails\n",
    "p_enhancement_given_no_spam = 0.001\n",
    "\n",
    "# Step 3: Compute total probability of \"enhancement\"\n",
    "# Law of total probability:\n",
    "# P(enhancement) = P(enhancement | spam)*P(spam) + P(enhancement | no spam)*P(no spam)\n",
    "p_enhancement = (p_enhancement_given_spam * p_spam) + (p_enhancement_given_no_spam * p_no_spam)\n",
    "\n",
    "# Step 4: Apply Bayes' Theorem\n",
    "# P(spam | enhancement) = [P(enhancement | spam) * P(spam)] / P(enhancement)\n",
    "p_spam_given_enhancement = (p_enhancement_given_spam * p_spam) / p_enhancement\n",
    "\n",
    "# Step 5: Print results in percentage form\n",
    "print(f\"P(spam) = {p_spam*100:.1f}%\")\n",
    "print(f\"P(enhancement | spam) = {p_enhancement_given_spam*100:.1f}%\")\n",
    "print(f\"P(enhancement | not spam) = {p_enhancement_given_no_spam*100:.2f}%\")\n",
    "print(f\"P(enhancement) = {p_enhancement*100:.2f}%\")\n",
    "print(f\"\\nP(spam | enhancement) = {p_spam_given_enhancement*100:.2f}%\")\n",
    "\n",
    "# Step 6: Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"If an email contains '{b}', there is a {p_spam_given_enhancement*100:.2f}% chance it is spam.\")\n",
    "print(f\"However, about {p_enhancement_given_no_spam*100:.2f}% of legitimate emails also contain '{b}',\")\n",
    "print(\"so a spam filter should not rely on a single word but combine multiple indicators for accuracy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a396a",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100b69a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "A **Naive Bayes classifier** is a type of **supervised machine learning algorithm** that applies **Bayes’ Theorem** to perform predictions and classifications.\n",
    "\n",
    "Recall Bayes’ Theorem:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "This formula calculates the probability of event \\( A \\) given that event \\( B \\) has occurred.\n",
    "\n",
    "---\n",
    "\n",
    "### From Bayes’ Theorem to Classification\n",
    "\n",
    "In classification tasks, we can think of:\n",
    "- \\( A \\) → a **class label** (e.g., \"spam\" or \"not spam\")\n",
    "- \\( B \\) → the **observed data** (e.g., the contents of an email)\n",
    "\n",
    "The model computes:\n",
    "- \\( P(\\text{spam} \\mid \\text{email}) \\)\n",
    "- \\( P(\\text{not spam} \\mid \\text{email}) \\)\n",
    "\n",
    "The classifier then predicts the class with the **higher posterior probability**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why It’s Called “Naive”\n",
    "\n",
    "The algorithm assumes that all features (e.g., words in an email) are **conditionally independent** given the class.  \n",
    "This simplification is rarely true in real-world data — hence the term *“naive”* — but it works surprisingly well for many problems like spam filtering, sentiment analysis, and text categorization.\n",
    "\n",
    "---\n",
    "\n",
    "### Why It’s Supervised\n",
    "\n",
    "Naive Bayes is **supervised** because it learns from **labeled training data**.  \n",
    "To estimate probabilities such as \\( P(\\text{spam}) \\) or \\( P(\\text{word} \\mid \\text{spam}) \\), we use previously tagged examples — for instance, a dataset where each email is labeled as spam or not spam.\n",
    "\n",
    "---\n",
    "\n",
    "In short, a **Naive Bayes classifier** uses Bayes’ Theorem with independence assumptions to predict which class a new data point most likely belongs to, based on learned probabilities from historical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4939f",
   "metadata": {},
   "source": [
    "## Investigate the Data\n",
    "\n",
    "In this lesson, we build a **Naive Bayes classifier** to predict whether a product review is **positive** or **negative**.  \n",
    "Such a model allows companies to quickly gauge public sentiment about a new product without manually reading thousands of reviews or social media posts.\n",
    "\n",
    "The dataset consists of **Amazon baby product reviews**.  \n",
    "Originally, it included multiple features such as the reviewer’s name, review date, and product rating.  \n",
    "For this task, only two key features are used:\n",
    "\n",
    "- The **text** of the review  \n",
    "- The **sentiment label** (“positive” or “negative”)\n",
    "\n",
    "Reviews with a score **below 4** are labeled as **negative**, while those with a score of **4 or higher** are labeled as **positive**.\n",
    "\n",
    "To keep training efficient, only a **small subset** of the dataset is loaded in the next lessons.  \n",
    "The **full dataset** will be used later when assembling the complete model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36e0b522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive 'crib' count: 1\n",
      "Negative 'crib' count: 12\n",
      "Top 5 positive words: [('to', 147), ('and', 130), ('the', 126), ('I', 119), ('a', 102)]\n",
      "Top 5 negative words: [('the', 309), ('to', 170), ('I', 157), ('and', 128), ('a', 110)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"data_folder\")\n",
    "\n",
    "# --- Load counters from pickle files ---\n",
    "with open(data_dir / \"pos_counter.pkl\", \"rb\") as f:\n",
    "    pos_counter: Counter = pickle.load(f)\n",
    "\n",
    "with open(data_dir / \"neg_counter.pkl\", \"rb\") as f:\n",
    "    neg_counter: Counter = pickle.load(f)\n",
    "\n",
    "# --- Example usage ---\n",
    "print(\"Positive 'crib' count:\", pos_counter[\"crib\"])\n",
    "print(\"Negative 'crib' count:\", neg_counter[\"crib\"])\n",
    "print(\"Top 5 positive words:\", pos_counter.most_common(5))\n",
    "print(\"Top 5 negative words:\", neg_counter.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af0283",
   "metadata": {},
   "source": [
    "## Bayes Theorem I\n",
    "\n",
    "In this lesson, we aim to build a classifier that predicts whether the review *\"This crib was amazing\"* is **positive** or **negative**.  \n",
    "To do so, we calculate the probabilities:\n",
    "\n",
    "$$\n",
    "P(\\text{positive} \\mid \\text{review}) \\quad \\text{and} \\quad P(\\text{negative} \\mid \\text{review})\n",
    "$$\n",
    "\n",
    "and determine which one is greater.\n",
    "\n",
    "We’ll use **Bayes’ Theorem** to compute these probabilities.  \n",
    "For example, for the positive case:\n",
    "\n",
    "$$\n",
    "P(\\text{positive} \\mid \\text{review}) = \\frac{P(\\text{review} \\mid \\text{positive}) \\cdot P(\\text{positive})}{P(\\text{review})}\n",
    "$$\n",
    "\n",
    "The first term we’ll focus on is **$P(\\text{positive})$**, which represents the probability that any given review in our dataset is positive.  \n",
    "To compute this, we count the total number of reviews labeled as positive and divide by the total number of reviews (both positive and negative).\n",
    "\n",
    "At this stage, our goal is to estimate this prior probability:\n",
    "\n",
    "$$\n",
    "P(\\text{positive})\n",
    "$$\n",
    "\n",
    "which forms part of the numerator in Bayes’ Theorem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7af7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "neg_list = pickle.load(open(\"data_folder/reviews_list_2.p\", \"rb\"))\n",
    "pos_list = pickle.load(open(\"data_folder/pos_list.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "beef5731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent positive: 0.5\n",
      "Percent negative: 0.5\n",
      "Sum (should be 1): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Find the total number of positive and negative reviews\n",
    "# len(pos_list) gives the total count of positive reviews\n",
    "# len(neg_list) gives the total count of negative reviews\n",
    "# Add them together to get the total number of reviews in the dataset\n",
    "\n",
    "total_reviews = len(pos_list) + len(neg_list)\n",
    "\n",
    "# Step 2: Calculate the proportion of positive and negative reviews\n",
    "# percent_pos = number of positive reviews divided by total reviews\n",
    "# percent_neg = number of negative reviews divided by total reviews\n",
    "# These represent P(positive) and P(negative) respectively\n",
    "percent_pos = len(pos_list) / total_reviews\n",
    "percent_neg = len(neg_list) / total_reviews\n",
    "\n",
    "# Step 3: Print the results to verify they sum to 1 (or very close to 1 due to rounding)\n",
    "print(\"Percent positive:\", percent_pos)\n",
    "print(\"Percent negative:\", percent_neg)\n",
    "print(\"Sum (should be 1):\", percent_pos + percent_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60068f",
   "metadata": {},
   "source": [
    "## Bayes Theorem II\n",
    "\n",
    "We continue classifying the review *“This crib was amazing”*.  \n",
    "In this step, we compute the second component of Bayes’ Theorem:\n",
    "\n",
    "$$\n",
    "P(\\text{positive} \\mid \\text{review}) = \\frac{P(\\text{review} \\mid \\text{positive}) \\cdot P(\\text{positive})}{P(\\text{review})}\n",
    "$$\n",
    "\n",
    "Here, we focus on the likelihood term:\n",
    "\n",
    "$$\n",
    "P(\\text{review} \\mid \\text{positive})\n",
    "$$\n",
    "\n",
    "This represents the probability of observing the exact review *given that it is positive*.  \n",
    "In other words, assuming the review is positive, how likely is it to contain exactly the words “This”, “crib”, “was”, and “amazing”?\n",
    "\n",
    "To simplify this computation, we make the **conditional independence assumption** — meaning that the appearance of one word does not depend on the presence of another.  \n",
    "Although this is a strong assumption, it is fundamental to the **Naive Bayes** approach.\n",
    "\n",
    "Under this assumption, we can expand the equation as follows:\n",
    "\n",
    "$$\n",
    "P(\\text{“This crib was amazing”} \\mid \\text{positive}) = P(\\text{“This”} \\mid \\text{positive}) \\cdot P(\\text{“crib”} \\mid \\text{positive}) \\cdot P(\\text{“was”} \\mid \\text{positive}) \\cdot P(\\text{“amazing”} \\mid \\text{positive})\n",
    "$$\n",
    "\n",
    "Each term, such as $P(\\text{“crib”} \\mid \\text{positive})$, represents the probability that the word “crib” appears in a positive review.  \n",
    "This can be calculated as:\n",
    "\n",
    "$$\n",
    "P(\\text{“crib”} \\mid \\text{positive}) = \\frac{\\text{# of times “crib” appears in positive reviews}}{\\text{total # of words in positive reviews}}\n",
    "$$\n",
    "\n",
    "By calculating this probability for every word in the review and multiplying them together, we obtain:\n",
    "\n",
    "$$\n",
    "P(\\text{review} \\mid \\text{positive})\n",
    "$$\n",
    "\n",
    "which forms the **likelihood** part of Bayes’ Theorem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1bff090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P(review | positive): 0.0\n",
      "P(review | negative): 0.0\n"
     ]
    }
   ],
   "source": [
    "review = \"This crib was amazing\"\n",
    "\n",
    "# Assuming equal prior probabilities for positive and negative reviews\n",
    "percent_pos = 0.5\n",
    "percent_neg = 0.5\n",
    "\n",
    "total_pos = sum(pos_counter.values())  # Total words in all positive reviews\n",
    "total_neg = sum(neg_counter.values())  # Total words in all negative reviews\n",
    "\n",
    "# Step 2: Initialize probabilities for positive and negative classes\n",
    "# These will accumulate the product of conditional probabilities for each word.\n",
    "pos_probability = 1\n",
    "neg_probability = 1\n",
    "\n",
    "# Step 3: Split the review text into individual words\n",
    "# Example: \"This crib was amazing\" → [\"This\", \"crib\", \"was\", \"amazing\"]\n",
    "review_words = review.split()\n",
    "\n",
    "# Step 4: Loop through each word in the review\n",
    "# For each word, we retrieve its count in the positive and negative review datasets.\n",
    "# Step 5: Multiply by the conditional probability P(word | positive)\n",
    "# Step 6: Repeat for the negative case\n",
    "for word in review_words:\n",
    "    word_in_pos = pos_counter[word]   # Number of times 'word' appears in positive reviews\n",
    "    word_in_neg = neg_counter[word]   # Number of times 'word' appears in negative reviews\n",
    "\n",
    "    pos_probability *= word_in_pos / total_pos  # Update positive probability\n",
    "    neg_probability *= word_in_neg / total_neg  # Update negative probability\n",
    "    \n",
    "print(\"\")\n",
    "print(\"P(review | positive):\", pos_probability)\n",
    "print(\"P(review | negative):\", neg_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5ea8178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_counter[\"crib\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379e248",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "In the previous step, we computed probabilities such as:\n",
    "\n",
    "$$\n",
    "P(\\text{“crib”} \\mid \\text{positive}) = \\frac{\\text{# of “crib” in positive reviews}}{\\text{# of words in positive reviews}}\n",
    "$$\n",
    "\n",
    "However, if the word *“crib”* never appears in any positive review, the numerator becomes 0.  \n",
    "Since the Naive Bayes classifier multiplies probabilities for all words, this would make the entire product:\n",
    "\n",
    "$$\n",
    "P(\\text{review} \\mid \\text{positive}) = 0\n",
    "$$\n",
    "\n",
    "This problem is especially common when the review contains **typos** or **unseen words** — words that do not exist in the training dataset.\n",
    "\n",
    "To address this, we apply a technique called **smoothing** (specifically, *Laplace smoothing*).  \n",
    "Smoothing prevents zero probabilities by adding a small constant (usually 1) to every word count and adjusting the denominator accordingly.\n",
    "\n",
    "The smoothed probability becomes:\n",
    "\n",
    "$$\n",
    "P(\\text{“crib”} \\mid \\text{positive}) = \\frac{\\text{# of “crib” in positive reviews} + 1}{\\text{# of words in positive reviews} + N}\n",
    "$$\n",
    "\n",
    "where **N** is the total number of **unique words** in the dataset.\n",
    "\n",
    "This ensures that no word has zero probability, improving the robustness of the model when classifying unseen or misspelled words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "458d76d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WITHOUT SMOOTHING ===\n",
      "P(review | positive): 0.0\n",
      "P(review | negative): 0.0\n",
      "\n",
      "=== WITH LAPLACE SMOOTHING ===\n",
      "P(review | positive): 1.0906857688451484e-12\n",
      "P(review | negative): 1.8834508880130966e-13\n"
     ]
    }
   ],
   "source": [
    "review = \"This cribb was amazing\"\n",
    "\n",
    "# Assuming equal priors for simplicity\n",
    "percent_pos = 0.5\n",
    "percent_neg = 0.5\n",
    "\n",
    "# Step 2: Calculate total word counts for both positive and negative datasets\n",
    "total_pos = sum(pos_counter.values())\n",
    "total_neg = sum(neg_counter.values())\n",
    "\n",
    "# Step 3: Initialize probabilities\n",
    "# We'll compute both versions: before and after smoothing\n",
    "pos_probability_no_smoothing = 1\n",
    "neg_probability_no_smoothing = 1\n",
    "\n",
    "pos_probability_smoothing = 1\n",
    "neg_probability_smoothing = 1\n",
    "\n",
    "# Step 4: Split the review into individual words\n",
    "review_words = review.split()\n",
    "\n",
    "# Step 5: Loop through each word to calculate probabilities\n",
    "for word in review_words:\n",
    "    word_in_pos = pos_counter[word]\n",
    "    word_in_neg = neg_counter[word]\n",
    "\n",
    "    # --- Without Smoothing ---\n",
    "    # If a word doesn't exist in the dataset, its count is 0 → probability becomes 0\n",
    "    pos_probability_no_smoothing *= word_in_pos / total_pos\n",
    "    neg_probability_no_smoothing *= word_in_neg / total_neg\n",
    "\n",
    "    # --- With Laplace Smoothing ---\n",
    "    # Add 1 to the numerator and total unique word count to the denominator\n",
    "    pos_probability_smoothing *= (word_in_pos + 1) / (total_pos + len(pos_counter))\n",
    "    neg_probability_smoothing *= (word_in_neg + 1) / (total_neg + len(neg_counter))\n",
    "\n",
    "# Step 6: Print results for comparison\n",
    "print(\"=== WITHOUT SMOOTHING ===\")\n",
    "print(\"P(review | positive):\", pos_probability_no_smoothing)\n",
    "print(\"P(review | negative):\", neg_probability_no_smoothing)\n",
    "\n",
    "print(\"\\n=== WITH LAPLACE SMOOTHING ===\")\n",
    "print(\"P(review | positive):\", pos_probability_smoothing)\n",
    "print(\"P(review | negative):\", neg_probability_smoothing)\n",
    "\n",
    "# Step 7: Observe how smoothing prevents zero probabilities\n",
    "# Even though \"cribb\" does not appear in the dataset, smoothing ensures\n",
    "# the overall product does not collapse to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c5b62",
   "metadata": {},
   "source": [
    "## Classify\n",
    "\n",
    "We have now computed both components of the numerator in Bayes’ Theorem:\n",
    "\n",
    "$$\n",
    "P(\\text{positive} \\mid \\text{review}) = \\frac{P(\\text{review} \\mid \\text{positive}) \\cdot P(\\text{positive})}{P(\\text{review})}\n",
    "$$\n",
    "\n",
    "Similarly, for the negative case:\n",
    "\n",
    "$$\n",
    "P(\\text{negative} \\mid \\text{review}) = \\frac{P(\\text{review} \\mid \\text{negative}) \\cdot P(\\text{negative})}{P(\\text{review})}\n",
    "$$\n",
    "\n",
    "Notice that the denominator, $P(\\text{review})$, is the same in both equations.  \n",
    "Since we only need to determine **which probability is greater**, we can ignore the denominator entirely.\n",
    "\n",
    "Therefore, the classification rule becomes:\n",
    "\n",
    "$$\n",
    "\\text{Predict} =\n",
    "\\begin{cases}\n",
    "\\text{Positive}, & \\text{if } P(\\text{review} \\mid \\text{positive}) \\cdot P(\\text{positive}) > P(\\text{review} \\mid \\text{negative}) \\cdot P(\\text{negative}) \\\\\n",
    "\\text{Negative}, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This means we simply compare the **numerators**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a312bc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final P(positive | review): 1.0906857688451484e-12\n",
      "Final P(negative | review): 1.2242430772085127e-12\n",
      "The review is negative\n"
     ]
    }
   ],
   "source": [
    "review = \"This crib was amazing\"\n",
    "\n",
    "# Prior probabilities (assuming equal likelihood for both classes)\n",
    "percent_pos = 0.5\n",
    "percent_neg = 0.5\n",
    "\n",
    "# Step 2: Calculate total word counts for each dataset\n",
    "total_pos = sum(pos_counter.values())\n",
    "total_neg = sum(neg_counter.values())\n",
    "\n",
    "# Step 3: Initialize likelihood probabilities\n",
    "pos_probability = 1\n",
    "neg_probability = 1\n",
    "\n",
    "# Step 4: Split the review into individual words\n",
    "review_words = review.split()\n",
    "\n",
    "# Step 5: Loop through each word in the review and compute smoothed conditional probabilities\n",
    "for word in review_words:\n",
    "    word_in_pos = pos_counter[word]  # Number of times word appears in positive reviews\n",
    "    word_in_neg = neg_counter[word]  # Number of times word appears in negative reviews\n",
    "\n",
    "    # Apply Laplace smoothing\n",
    "    pos_probability *= (word_in_pos + 1) / (total_pos + len(pos_counter))\n",
    "    neg_probability *= (word_in_neg + 1) / (total_neg + len(neg_counter))\n",
    "\n",
    "# Step 6: Multiply by prior probabilities to get final class probabilities\n",
    "final_pos = pos_probability * percent_pos\n",
    "final_neg = neg_probability * percent_neg\n",
    "\n",
    "# Step 7: Print computed final probabilities\n",
    "print(\"Final P(positive | review):\", final_pos)\n",
    "print(\"Final P(negative | review):\", final_neg)\n",
    "\n",
    "# Step 8: Compare and classify the review\n",
    "# The class with the higher probability is the predicted label\n",
    "if final_pos > final_neg:\n",
    "    print(\"The review is positive\")\n",
    "else:\n",
    "    print(\"The review is negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4839647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final P(positive | review): 1.0906857688451484e-12\n",
      "Final P(negative | review): 1.2242430772085127e-12\n",
      "The review is negative\n"
     ]
    }
   ],
   "source": [
    "review = \"This crib was terrible\"\n",
    "\n",
    "# Step 2: Define equal prior probabilities for both classes\n",
    "percent_pos = 0.5\n",
    "percent_neg = 0.5\n",
    "\n",
    "# Step 3: Calculate total word counts for each sentiment dataset\n",
    "total_pos = sum(pos_counter.values())\n",
    "total_neg = sum(neg_counter.values())\n",
    "\n",
    "# Step 4: Initialize likelihood probabilities\n",
    "pos_probability = 1\n",
    "neg_probability = 1\n",
    "\n",
    "# Step 5: Split the review into individual words\n",
    "review_words = review.split()\n",
    "\n",
    "# Step 6: Loop through each word and apply Laplace smoothing\n",
    "# For each word, we calculate P(word | positive) and P(word | negative)\n",
    "for word in review_words:\n",
    "    word_in_pos = pos_counter[word]  # Word frequency in positive reviews\n",
    "    word_in_neg = neg_counter[word]  # Word frequency in negative reviews\n",
    "\n",
    "    # Apply Laplace smoothing to avoid zero probabilities\n",
    "    pos_probability *= (word_in_pos + 1) / (total_pos + len(pos_counter))\n",
    "    neg_probability *= (word_in_neg + 1) / (total_neg + len(neg_counter))\n",
    "\n",
    "# Step 7: Combine with prior probabilities to compute final class probabilities\n",
    "final_pos = pos_probability * percent_pos\n",
    "final_neg = neg_probability * percent_neg\n",
    "\n",
    "# Step 8: Display the computed probabilities for comparison\n",
    "print(\"Final P(positive | review):\", final_pos)\n",
    "print(\"Final P(negative | review):\", final_neg)\n",
    "\n",
    "# Step 9: Classify the review based on which probability is greater\n",
    "if final_pos > final_neg:\n",
    "    print(\"The review is positive\")\n",
    "else:\n",
    "    print(\"The review is negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241b419f",
   "metadata": {},
   "source": [
    "## Formatting the Data for scikit-learn\n",
    "\n",
    "Congratulations! You’ve built your own Naive Bayes text classifier.\n",
    "If you have a dataset of text labeled with different classes, your classifier can now predict which class a new document belongs to.\n",
    "\n",
    "We’ll now explore how Python’s scikit-learn library can handle this process for us automatically.\n",
    "\n",
    "Transforming Text into Numerical Data\n",
    "\n",
    "To use scikit-learn’s Naive Bayes classifier, we must first transform our text data into a numerical format.\n",
    "This is achieved using CountVectorizer, which converts text into a matrix of word counts — often referred to as the bag-of-words model.\n",
    "\n",
    "### Step 1: Create and Fit the Vectorizer\n",
    "\n",
    "We begin by creating a CountVectorizer and fitting it to our training data.\n",
    "The .fit() method learns the vocabulary of the training set.\n",
    "After fitting, the vectorizer learns which words are part of the vocabulary, for example:\n",
    "\n",
    "{'training': 3, 'review': 1, 'one': 0, 'second': 2}\n",
    "\n",
    "\n",
    "### Step 2: Transform Text into Word Counts\n",
    "\n",
    "Once trained, we can use .transform() to convert new text into numerical count vectors.\n",
    "This produces an array such as:\n",
    "\n",
    "[[1 2 0 0]]\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "“one” appears once\n",
    "\n",
    "“review” appears twice\n",
    "\n",
    "“training” and “second” do not appear\n",
    "\n",
    "\n",
    "### Step 3: Understanding the Vocabulary\n",
    "\n",
    "You can print vectorizer.vocabulary_ to view the mapping between each word and its corresponding index.\n",
    "Each index corresponds to a column in the count matrix, defining the word’s position.\n",
    "\n",
    "If a new word (like “two”) was not present in the data used for .fit(), it will not appear in the vocabulary and will therefore be ignored during transformation.\n",
    "\n",
    "\n",
    "### Step 4: Using the Count Data in Naive Bayes\n",
    "\n",
    "The counts matrix is now ready to be used as input for a Naive Bayes classifier such as MultinomialNB.\n",
    "The typical workflow is as follows:\n",
    "\n",
    "Fit the CountVectorizer on the training text\n",
    "\n",
    "Transform the text into word count vectors\n",
    "\n",
    "Train the Naive Bayes classifier using those vectors\n",
    "\n",
    "Transform new text data and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41049604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary learned by CountVectorizer:\n",
      "{'wanted': 1521, 'to': 1429, 'love': 805, 'this': 1408, 'but': 182, 'it': 712, 'was': 1525, 'pretty': 1056, 'expensive': 467, 'for': 525, 'only': 951, 'few': 495, 'months': 871, 'worth': 1584, 'of': 937, 'calendar': 187, 'pages': 981, 'ended': 434, 'up': 1486, 'buying': 185, 'regular': 1130, 'weekly': 1541, 'planner': 1024, '55': 11, 'off': 938, 'the': 1393, 'that': 1392, 'is': 709, '11': 2, 'and': 63, 'has': 618, 'all': 47, 'seven': 1219, 'days': 339, 'on': 947, 'right': 1163, 'page': 980, 'left': 765, 'room': 1166, 'write': 1588, 'do': 380, 'list': 785, 'goals': 577, 'found': 539, 'be': 120, 'more': 873, 'helpful': 633, 'because': 123, 'could': 306, 'mark': 823, 'each': 409, 'day': 337, 'eating': 417, 'sleeping': 1252, 'blocks': 149, 'then': 1397, 'also': 55, 'see': 1207, 'them': 1395, 'side': 1235, 'by': 186, 'her': 636, 'patterns': 993, 'easily': 413, 'with': 1568, 'view': 1511, 'cute': 328, 'just': 724, 'not': 919, 'what': 1550, 'like': 778, 'log': 792, 'think': 1405, 'would': 1585, 'work': 1576, 'better': 137, 'clearer': 243, 'am': 59, 'pm': 1034, 'sections': 1205, '12': 3, 'hours': 661, 'so': 1270, 'you': 1598, 'really': 1113, 'need': 903, 'two': 1474, 'if': 673, 'your': 1600, 'baby': 104, 'feeds': 490, 'or': 959, 'wets': 1549, 'lot': 803, 'in': 681, 'early': 411, 'morning': 874, 'between': 138, 'midnight': 852, '7am': 14, 'we': 1539, 're': 1104, 'cramming': 315, 'those': 1409, 'blank': 146, 'spaces': 1289, 'above': 19, 'now': 926, 'my': 888, 'wife': 1561, 'have': 623, 'six': 1243, 'month': 870, 'old': 945, 'boy': 164, 'around': 81, 'decided': 342, 'she': 1224, 'return': 1155, 'instead': 696, 'being': 132, 'stay': 1314, 'at': 92, 'home': 652, 'mom': 866, 'hired': 644, 'an': 61, 'nanny': 891, 'care': 194, 'our': 966, 'little': 787, 'arrangement': 82, 'worked': 1577, 'quite': 1097, 'well': 1544, 'ever': 452, 'since': 1240, 'shortly': 1230, 'after': 40, 'starting': 1308, 'realized': 1111, 'some': 1276, 'sort': 1285, 'journal': 723, 'track': 1446, 'activities': 28, 'while': 1556, 'he': 627, 'were': 1546, 'working': 1579, 'used': 1492, 'plain': 1022, 'notebook': 921, 'period': 1005, 'weeks': 1542, 'until': 1485, 'stumbled': 1336, 'tracker': 1447, 'daily': 332, 'childcare': 227, 'layout': 755, 'use': 1491, 'excellent': 460, 'idea': 671, 'are': 78, 'clearly': 244, 'divided': 379, 'into': 703, 'columns': 253, 'tracking': 1449, 'feedings': 489, 'nap': 892, 'time': 1425, 'diaper': 360, 'changes': 217, 'play': 1026, 'as': 86, 'general': 559, 'areas': 80, 'notes': 922, 'milestones': 855, 'legibility': 766, 'huge': 667, 'improvement': 679, 'over': 971, 'standard': 1303, 'entries': 445, 'becoming': 125, 'small': 1260, 'paragraphs': 987, 'short': 1229, 'moments': 867, 'can': 191, 'summarize': 1347, 'data': 335, 'totals': 1442, 'section': 1204, 'determine': 355, 'key': 730, 'information': 693, 'how': 665, 'much': 885, 'did': 364, 'eat': 415, 'bowel': 161, 'movement': 883, 'sleep': 1250, 'they': 1401, 'get': 564, 'etc': 449, 'there': 1399, 'however': 666, 'frustrating': 549, 'limitations': 781, 'first': 510, 'entire': 443, 'about': 18, 'half': 602, 'sheet': 1225, 'down': 392, 'middle': 851, 'portrait': 1044, 'constrains': 285, 'very': 1505, 'column': 252, 'row': 1172, 'okay': 944, 'summarized': 1348, 'ounces': 965, '34': 9, 'once': 948, 'becomes': 124, 'active': 27, 'know': 737, 'than': 1390, 'tummy': 1469, 'under': 1479, 'things': 1404, 'start': 1306, 'tight': 1423, 'another': 65, 'problem': 1067, 'covers': 313, 'out': 968, 'fine': 505, 'intention': 699, 'child': 225, 'providers': 1079, 'which': 1555, 'often': 941, 'starts': 1309, 'earlier': 410, '6am': 13, 'using': 1498, 'require': 1142, 'second': 1201, 'good': 581, 'easy': 414, 'read': 1106, 'instantly': 695, 'gather': 557, 'matter': 832, 'high': 639, 'quality': 1092, 'paper': 986, 'consistent': 282, 'don': 388, 'gets': 565, 'most': 875, 'less': 768, 'one': 949, 'babies': 103, 'cover': 312, 'thick': 1402, 'hardback': 616, 'bends': 135, 'bag': 110, 'should': 1231, 'bigger': 141, 'understand': 1480, 'portability': 1043, 'concern': 271, 'current': 326, 'size': 1244, 'entirely': 444, 'too': 1434, 'conclusion': 273, 'making': 817, 'own': 975, 'format': 533, 'spreadsheet': 1296, 'includes': 685, '24': 7, 'hour': 660, 'space': 1288, 'comments': 259, 'along': 53, 'other': 964, 'had': 601, 'bound': 160, 'cheaply': 222, 'local': 791, 'shop': 1228, 'adequate': 34, 'thought': 1411, 'keeping': 727, 'simple': 1238, 'handwritten': 608, 'nice': 913, 'haven': 624, 'thing': 1403, 'here': 637, 'why': 1560, 'when': 1551, 'breastfeeding': 171, 'phone': 1009, 'close': 245, 'keep': 726, 'yourself': 1601, 'entertained': 442, 'able': 17, 'grab': 583, 'both': 155, 'pen': 999, 'consistently': 283, 'skilled': 1247, 'me': 836, 'nurse': 928, 'same': 1179, 'place': 1019, 'every': 453, 'deprived': 351, 'least': 762, 'forget': 528, 'look': 796, 'started': 1307, 'perfect': 1002, 'app': 73, 'either': 425, 'mindlessly': 858, 'hit': 647, 'button': 183, 'connect': 276, 'gives': 571, 'example': 459, 'tell': 1385, 'long': 794, 'average': 98, 'been': 127, 'taking': 1370, 'training': 1452, 'nursed': 929, 'him': 641, '177': 5, 'times': 1427, 'last': 749, 'granted': 588, 'serves': 1215, 'no': 917, 'useful': 1493, 'purpose': 1086, 'feeling': 492, 'perverse': 1008, 'satisfaction': 1181, 'adorable': 36, 'book': 151, 'pieces': 1016, 'attached': 95, 'activity': 29, 'several': 1220, 'though': 1410, 'sew': 1221, 'make': 815, 'directed': 369, 'will': 1563, 'realize': 1110, 'priced': 1061, 'hard': 615, 'age': 42, 'group': 595, 'playing': 1027, 'teether': 1381, 'ridiculous': 1162, 'clamp': 237, 'daughter': 336, 'going': 579, 'vibrating': 1507, 'mouth': 878, 'big': 140, 'awkward': 101, 'push': 1088, 'money': 869, 'opinion': 957, 'product': 1070, 'case': 202, 'made': 812, 'bite': 144, 'vibration': 1508, 'does': 385, 'toy': 1444, 'bottom': 158, '5mo': 12, 'who': 1557, 'loves': 807, 'tap': 1373, 'toys': 1445, 'his': 645, 'yes': 1595, 'sounds': 1287, 'weird': 1543, 'husband': 668, 'drummer': 401, 'got': 582, 'obsessed': 933, 'tapping': 1376, 'even': 450, 'drinking': 397, 'bottle': 156, 'try': 1467, 'hands': 607, 'feel': 491, 'mini': 860, 'dodge': 384, 'ball': 114, 'sticks': 1320, 'face': 476, 'happy': 614, 'bought': 159, 'infant': 690, 'gum': 599, 'massager': 829, 'electric': 427, 'toothbrush': 1437, 'always': 58, 'interested': 700, 'brush': 179, 'teeth': 1380, 'supervised': 1351, 'doesn': 386, 'choke': 231, 'skinny': 1248, 'objects': 932, 'guard': 597, 'block': 148, 'prevent': 1057, 'any': 67, 'type': 1475, 'choking': 232, 'enough': 439, 'turn': 1470, 'again': 41, 'hold': 648, 'trying': 1468, 'find': 503, 'vibrations': 1509, 'himself': 642, 'sucks': 1339, 'chew': 223, 'teething': 1384, 'bites': 145, 'never': 909, 'dumb': 405, 'vibrate': 1506, 'disappointing': 372, 'liked': 779, 'massaging': 830, 'action': 26, 'took': 1435, 'finally': 502, 'battery': 119, 'gave': 558, 'nearly': 897, 'spurts': 1298, 'rest': 1149, 'wouldn': 1586, 'waste': 1531, 'great': 590, 'freaks': 541, 'wants': 1523, 'nothing': 923, 'might': 853, 'wonderful': 1574, 'appears': 75, 'hated': 621, 'cannot': 192, 'comment': 258, 'effectiveness': 421, 'bit': 143, 'seemed': 1208, 'cool': 299, 'passed': 991, 'friend': 544, 'recommended': 1118, 'throwing': 1417, 'floor': 520, 'hopefully': 656, 'squeeze': 1299, 'help': 631, 'heavy': 629, 'seems': 1209, 'enjoy': 438, 'way': 1537, 'feels': 493, 'actually': 31, 'tired': 1428, 'minute': 861, 'maybe': 835, 'older': 946, 'teethers': 1382, 'twin': 1472, 'boys': 165, 'years': 1594, 'ago': 43, 'absolutely': 21, 'loved': 806, 'new': 910, 'sought': 1286, 'surprised': 1357, 'arrived': 84, 'packaging': 978, 'stated': 1311, 'safety': 1176, 'tested': 1389, 'bpa': 166, 'lead': 757, 'phthalates': 1011, 'state': 1310, 'anywhere': 71, 'package': 977, 'free': 542, 'former': 534, 'regulator': 1131, 'medical': 841, 'devices': 359, 'sensitive': 1212, 'company': 264, 'labeling': 741, 'statements': 1312, 'perhaps': 1004, 'rather': 1102, 'err': 446, 'caution': 208, 'where': 1553, 'concerned': 272, 'lack': 742, 'clear': 242, 'pause': 994, 'specifically': 1292, 'test': 1388, 'acceptable': 22, 'amount': 60, 'allowed': 50, 'simply': 1239, 'label': 740, 'phthalate': 1010, 'eight': 424, 'sons': 1282, 'heard': 628, 'put': 1089, 'manner': 819, 'plastic': 1025, 'their': 1394, 'mouths': 879, 'sure': 1356, 'riddled': 1161, 'strongly': 1332, 'suspected': 1358, 'dangerous': 333, 'conscience': 279, 'allow': 49, 'anything': 70, 'may': 834, 'contain': 288, 'someone': 1277, 'site': 1241, 'indicated': 686, 'representative': 1141, 'from': 547, 'told': 1433, 'still': 1321, 'skeptical': 1246, 'such': 1337, 'companies': 263, 'proudly': 1077, 'products': 1071, 'language': 746, 'confused': 275, 'son': 1281, 'suck': 1338, 'fingers': 506, 'messaging': 848, 'corn': 301, 'squeezed': 1300, 'order': 960, 'young': 1599, 'already': 54, 'isn': 710, 'reason': 1114, 'pick': 1012, 'car': 193, 'stuffed': 1335, 'different': 367, 'complaint': 266, 'star': 1304, 'bead': 121, 'end': 433, 'pointy': 1038, 'catches': 205, 'sore': 1284, 'coming': 257, 'wrong': 1591, 'cry': 324, 'colors': 251, 'definitely': 345, 'catch': 204, 'eye': 473, 'overall': 972, 'ok': 943, 'price': 1060, 'necklace': 902, 'weren': 1547, 'reviewer': 1157, 'pointed': 1037, 'chews': 224, 'edge': 420, 'painful': 984, 'thrown': 1418, 'having': 626, 'pull': 1080, 'road': 1164, 'week': 1540, 'screamed': 1194, 'suddenly': 1340, 'back': 106, 'crying': 325, 'hysterically': 669, 'sling': 1255, 'seen': 1210, 'mothers': 877, 'these': 1400, 'before': 128, 'childbirth': 226, 'class': 238, 'raved': 1103, 'excited': 461, 'received': 1115, 'gift': 567, 'cradle': 314, 'position': 1045, 'afraid': 39, 'sufficate': 1341, 'fabric': 474, 'hand': 603, 'next': 912, 'easier': 412, 'grocery': 594, 'store': 1325, 'pulled': 1081, 'shoulder': 1232, 'crawling': 316, 'pulling': 1082, 'restrained': 1151, 've': 1503, 'tried': 1462, 'kangaroo': 725, 'scared': 1188, 'fall': 479, 'wiggle': 1562, 'worm': 1582, 'strap': 1328, 'neck': 901, 'interfere': 701, 'circulation': 236, 'glad': 573, 'rethink': 1153, 'kind': 733, 'carrier': 198, 'slings': 1256, 'practiced': 1050, 'cat': 203, 'comfortable': 256, 'must': 887, 'admit': 35, 'front': 548, 'packs': 979, 'cut': 327, 'across': 25, 'go': 576, 'carry': 199, 'screams': 1195, 'hardly': 617, 'cries': 320, 'say': 1187, 'mean': 838, 'turns': 1471, 'red': 1123, 'soon': 1283, 'take': 1366, 'pack': 976, 'honestly': 653, 'lay': 754, 'cooperates': 300, 'carried': 197, 'brought': 178, 'hospital': 659, 'walking': 1519, 'plan': 1023, 'hip': 643, 'watched': 1534, 'video': 1510, 'support': 1354, 'hope': 654, 'stick': 1318, 'traditional': 1451, 'larger': 748, 'women': 1572, 'endowed': 435, 'purchased': 1085, 'total': 1441, 'thinking': 1406, 'needed': 904, 'change': 215, 'bedding': 126, 'constantly': 284, 'due': 404, 'spit': 1295, 'instructions': 697, 'said': 1177, 'hang': 610, 'dry': 402, 'through': 1414, 'toss': 1440, 'dryer': 403, 'mattress': 833, 'known': 739, 'didn': 365, 'extra': 471, 'went': 1545, 'cosleeper': 304, 'material': 831, 'special': 1290, 'holds': 650, 'lots': 804, 'washes': 1527, 'uses': 1497, 'shipping': 1227, 'quick': 1095, 'sheets': 1226, 'themselves': 1396, 'soft': 1271, 'course': 311, 'inexpensive': 689, 'expected': 464, 'five': 513, 'co': 246, 'sleeper': 1251, 'assembled': 90, 'yet': 1596, 'box': 163, 'torn': 1439, 'dented': 348, 'looking': 798, 'contents': 292, 'looks': 799, 'law': 753, 'puts': 1090, 'together': 1432, 'stroller': 1331, 'won': 1573, 'fit': 511, 'graco': 585, 'straps': 1329, 'tie': 1421, 'trays': 1457, 'came': 190, 'un': 1478, 'sewed': 1222, 'buy': 184, 'happened': 612, 'unforutnately': 1481, 'travel': 1456, 'system': 1361, 'carseat': 201, 'grace': 584, 'marathon': 822, 'useless': 1495, 'quattro': 1093, 'metrolite': 850, 'complete': 268, 'primarily': 1062, 'takes': 1369, 'trash': 1455, 'bags': 111, 'supposed': 1355, 'couple': 309, 'quickly': 1096, 'discovered': 374, 'pail': 982, 'full': 550, 'dropping': 399, 'mechanism': 840, 'stuck': 1334, 'forcing': 527, 'clean': 240, 'whenever': 1552, 'wipe': 1564, 'messy': 849, 'load': 790, 'top': 1438, 'routinely': 1170, 'flip': 518, 'dump': 406, 'dirty': 370, 'lift': 774, 'whole': 1559, 'lid': 770, 'allows': 51, 'tabs': 1364, 'getting': 566, 'trap': 1454, 'smell': 1262, 'negates': 906, 'makes': 816, 'something': 1278, 'cheaper': 221, 'bad': 108, 'proof': 1074, 'fact': 477, 'empty': 432, 'leaks': 758, 'pleasant': 1029, 'truly': 1466, 'believe': 133, 'best': 136, 'market': 824, 'garbage': 555, 'fantastic': 481, 'job': 719, 'containing': 290, 'odors': 936, 'couldn': 307, 'biggest': 142, 'kitchen': 734, 'sized': 1245, 'champ': 214, 'requires': 1144, 'us': 1490, 'remove': 1138, 'wasteful': 1532, 'solution': 1275, 'smaller': 1261, 'accomodate': 23, 'common': 260, 'household': 663, 'tall': 1372, 'efficiently': 422, 'point': 1036, 'separate': 1213, 'disguise': 376, 'doubt': 391, 'expend': 466, 'effort': 423, 'works': 1580, 'piston': 1018, 'drop': 398, 'joint': 720, 'meet': 844, 'thus': 1419, 'caught': 207, 'its': 716, 'replacement': 1140, 'moved': 882, 'drum': 400, 'foot': 524, 'securing': 1206, 'handed': 604, 'operation': 956, 'states': 1313, 'third': 1407, 'trip': 1463, 'ask': 87, 'china': 230, 'contains': 291, 'category': 206, 'throw': 1416, 'soiled': 1272, 'ours': 967, 'tends': 1387, 'lazy': 756, 'fill': 499, 'yuck': 1602, 'save': 1182, 'diapers': 362, 'reading': 1107, 'reviews': 1159, 'line': 782, 'chose': 234, 'register': 1127, 'disappointed': 371, 'sense': 1211, 'super': 1350, 'hero': 638, 'smelly': 1265, 'enter': 440, 'corner': 302, 'guess': 598, 'grateful': 589, 'house': 662, 'poopie': 1040, 'wet': 1548, 'scented': 1189, 'away': 99, 'garabage': 554, 'taken': 1367, 'opening': 954, 'difficult': 368, 'blue': 150, 'round': 1168, 'part': 990, 'broke': 176, 'masking': 827, 'tape': 1374, 'thrilled': 1413, 'specific': 1291, 'cheap': 220, 'smells': 1264, 'large': 747, 'far': 482, 'gab': 553, 'nature': 895, 'awesome': 100, 'stinks': 1322, 'neat': 898, 'disposal': 377, 'shower': 1233, 'handle': 606, 'purchase': 1084, 'liners': 784, 'mistake': 864, 'stars': 1305, 'parents': 989, 'children': 228, '14': 4, 'come': 254, 'odor': 934, 'literally': 786, 'smelled': 1263, 'poop': 1039, 'stand': 1302, 'changed': 216, 'saving': 1184, 'plus': 1033, 'infants': 691, 'want': 1520, 'consider': 281, 'else': 428, 'decor': 343, 'gifts': 568, 'favorite': 483, 'positive': 1046, 'open': 952, 'without': 1570, 'breaking': 168, 'nail': 890, 'changing': 218, 'depending': 349, 'waited': 1515, 'door': 390, 'knot': 736, 'brim': 173, 'dispose': 378, 'squirmy': 1301, 'table': 1363, 'registered': 1128, 'item': 714, 'impressed': 677, 'jammed': 718, 'leaves': 764, 'faint': 478, 'live': 788, 'hassle': 620, 'beware': 139, 'value': 1501, 'manicures': 818, 'refills': 1126, 'pleased': 1031, 'diet': 366, 'bowl': 162, 'movements': 884, 'longer': 795, 'contained': 289, 'tightly': 1424, 'wrap': 1587, 'washing': 1528, 'frequently': 543, 'stench': 1317, 'comes': 255, 'maintain': 813, 'serious': 1214, 'design': 352, 'flaw': 516, 'tapered': 1375, 'noticeably': 925, 'probably': 1066, 'balancing': 113, 'purposes': 1087, 'pros': 1075, 'alternatives': 57, 'normal': 918, 'bagscons': 112, 'glowing': 575, 'receiving': 1116, 'replace': 1139, 'broken': 177, 'genie': 560, 'disappointment': 373, 'complaints': 267, 'unsanitary': 1483, 'putting': 1091, 'wipes': 1565, 'slot': 1257, 'dumping': 407, 'bucket': 180, 'germy': 563, 'mess': 847, 'unrealistic': 1482, 'fear': 484, 'toddler': 1431, 'near': 896, 'germs': 562, 'let': 769, 'frightening': 546, 'experience': 468, 'yikes': 1597, 'prevention': 1058, 'prepared': 1055, 'opened': 953, 'seals': 1198, 'individual': 688, 'sea': 1196, 'ones': 950, 'certainly': 211, 'arrives': 85, 'year': 1593, 'll': 789, 'cleaner': 241, 'involved': 707, 'version': 1504, 'operate': 955, 'model': 865, 'cross': 321, 'stays': 1316, 'friends': 545, 'houses': 664, 'fought': 538, 'genies': 561, 'warm': 1524, 'appeared': 74, 'lysol': 811, 'various': 1502, 'tricks': 1460, 'mask': 826, 'emanating': 429, 'luck': 809, 'breast': 169, 'fed': 486, 'imagine': 674, 'solids': 1274, 'ick': 670, 'wondering': 1575, 'fighting': 496, 'option': 958, 'air': 45, 'conditioning': 274, 'summer': 1349, 'reviewers': 1158, 'complained': 265, 'completely': 269, 'convenient': 298, 'drag': 395, 'extremely': 472, 'horrible': 658, 'please': 1030, 'figured': 498, 'protection': 1076, 'desired': 353, 'badly': 109, 'multiple': 886, 'nasty': 894, 'spew': 1294, 'forth': 536, 'sealed': 1197, 'somewhat': 1280, 'correctly': 303, 'concept': 270, 'fabulous': 475, 'cons': 278, 'area': 79, 'addition': 33, 'odorless': 935, 'notice': 924, 'worse': 1583, 'cost': 305, 'savings': 1185, 'began': 129, 'cylinder': 329, 'continually': 293, 'fun': 551, 'apart': 72, '00': 0, 'placed': 1020, 'hole': 651, 'sometimes': 1279, 'dealing': 341, 'flipped': 519, 'fell': 494, 'task': 1377, 'retrieve': 1154, 'figure': 497, 'fix': 514, '1st': 6, 'hauled': 622, 'pickup': 1014, 'lasted': 750, 'recommend': 1117, 'newborn': 911, 'within': 1569, 'liner': 783, 'assume': 91, 'stickies': 1319, 'anymore': 68, 'groceries': 593, 'perfectly': 1003, 'deal': 340, 'babi': 102, 'italia': 713, 'pinehurst': 1017, 'classic': 239, 'crib': 318, 'ultimate': 1476, 'backup': 107, 'wash': 1526, 'problems': 1068, 'fitting': 512, 'rails': 1099, 'forced': 526, 'attach': 94, 'snaps': 1269, 'stretch': 1330, 'flat': 515, 'today': 1430, 'marks': 825, 'paint': 985, 'ah': 44, 'improvised': 680, 'rail': 1098, 'length': 767, 'actual': 30, 'usually': 1499, 'teethes': 1383, 'based': 116, 'bargains': 115, 'expectations': 463, 'night': 914, 'unsnap': 1484, '10': 1, 'places': 1021, 'engage': 437, 'bumpers': 181, 'elastic': 426, 'slats': 1249, 'attractive': 96, 'miracle': 863, 'hoped': 655, 'trouble': 1464, 'lifting': 775, 'lightweight': 777, 'foam': 521, 'result': 1152, 'seconds': 1203, 'snapping': 1268, 'ties': 1422, 'bending': 134, 'solid': 1273, 'minutes': 862, 'manufacturer': 820, 'recommends': 1119, 'means': 839, 'snap': 1267, 'suggest': 1343, 'coil': 249, 'center': 209, 'feeding': 488, 'schedule': 1190, 'life': 771, 'doctor': 382, 'questions': 1094, 'habits': 600, 'saver': 1183, 'trends': 1459, 'answer': 66, 'pediatrician': 996, 'communicate': 261, 'everyone': 455, 'required': 1143, 'leave': 763, 'finish': 507, 'haves': 625, 'helps': 635, 'exactly': 458, 'gone': 580, 'mother': 876, 'watching': 1535, 'happier': 613, 'routine': 1169, 'sitter': 1242, 'helped': 632, 'prepare': 1054, 'evening': 451, 'likely': 780, 'sick': 1234, 'many': 821, 'producing': 1069, 'dehydrated': 346, 'note': 920, 'writes': 1589, 'whether': 1554, 'lunch': 810, 'playtime': 1028, 'included': 684, 'walk': 1517, 'moms': 868, 'wanting': 1522, 'kids': 731, 'dads': 331, 'lol': 793, 'alternative': 56, 'printing': 1064, 'searching': 1199, 'crumpled': 323, 'piece': 1015, 'previous': 1059, 'preferred': 1053, 'held': 630, 'basics': 117, 'wish': 1566, 'struggle': 1333, 'caretaker': 196, 'wrote': 1592, 'spend': 1293, 'neighbor': 907, 'loosely': 801, 'developing': 356, 'milk': 856, 'cohesion': 248, 'visits': 1514, 'brand': 167, 'books': 152, 'absolute': 20, 'trackers': 1448, 'available': 97, 'naps': 893, 'tracks': 1450, 'nights': 915, 'important': 676, 'during': 408, 'caregiver': 195, 'postpartum': 1047, 'nurses': 930, 'urination': 1489, 'remind': 1137, 'overwhelmed': 973, 'remember': 1135, 'cried': 319, 'major': 814, 'contact': 287, 'call': 188, 'trend': 1458, 'indication': 687, 'suggested': 1344, 'pumping': 1083, 'supplement': 1352, 'formula': 535, 'sufficient': 1342, 'water': 1536, 'drink': 396, 'supply': 1353, 'breastmilk': 172, 'wasn': 1530, 'giving': 572, 'gas': 556, 'ate': 93, 'food': 522, 'allergy': 48, 'peanut': 995, 'family': 480, 'smile': 1266, 'visit': 1512, 'ect': 419, 'memory': 846, 'babysitter': 105, 'grandma': 586, 'goes': 578, 'recorded': 1121, 'diary': 363, 'certain': 210, 'suit': 1345, 'rough': 1167, 'refer': 1124, 'forgot': 529, 'woke': 1571, 'emergency': 431, 'consent': 280, 'form': 531, 'needs': 905, 'immunizations': 675, 'info': 692, 'glance': 574, 'developmental': 358, 'organized': 963, 'create': 317, 'spreadsheets': 1297, 'people': 1000, 'practical': 1049, 'pees': 998, 'poops': 1041, 'breastfeed': 170, 'especially': 448, 'adults': 37, 'sharing': 1223, 'responsibilities': 1148, 'wake': 1516, 'eaten': 416, 'written': 1590, 'record': 1120, 'dr': 394, 'appts': 77, 'analyze': 62, 'asks': 89, 'urinating': 1488, 'realizing': 1112, 'oh': 942, 'hasn': 619, 'slept': 1254, 'number': 927, 'done': 389, 'looked': 797, 'pattern': 992, 'routines': 1171, 'jot': 721, 'knew': 735, 'beginning': 130, 'transfer': 1453, 'everything': 456, 'funny': 552, 'forgotten': 530, 'born': 154, 'pee': 997, 'poopy': 1042, 'asking': 88, 'per': 1001, 'bring': 174, 'color': 250, 'code': 247, 'pottied': 1048, 'ordering': 961, 'ummmm': 1477, 'pain': 983, 'killers': 732, 'mass': 828, 'hormones': 657, 'slowly': 1259, 'lst': 808, 'minds': 859, 'handy': 609, 'eats': 418, 'discuss': 375, 'progress': 1072, 'grandparents': 587, 'watch': 1533, 'doing': 387, 'highly': 640, 'continue': 294, 'delivery': 347, 'saw': 1186, 'ends': 436, 'swear': 1360, 'throughout': 1415, 'mile': 854, 'stone': 1323, '8230': 16, 'outings': 969, 'temperature': 1386, 'readings': 1108, 'calls': 189, 'stopped': 1324, 'swaddling': 1359, 'resource': 1146, 'later': 751, 'formally': 532, 'album': 46, 'valuable': 1500, 'tidbits': 1420, 'memories': 845, 'rummage': 1173, 'sale': 1178, 'helping': 634, 'ex': 457, 'visiting': 1513, 'almost': 52, 'filled': 500, 'development': 357, 'story': 1327, 'dad': 330, 'connected': 277, 'everyday': 454, 'medicine': 842, 'give': 569, 'rundown': 1175, 'three': 1412, 'issues': 711, 'necessary': 899, 'explain': 469, 'behavior': 131, 'lactation': 743, 'consultants': 286, 'contributed': 296, 'deprivation': 350, 'dark': 334, 'respond': 1147, 'referring': 1125, 'tasks': 1378, 'include': 683, 'finished': 508, 'knowing': 738, 'meal': 837, 'predicting': 1051, 'communication': 262, 'childs': 229, 'learn': 760, 'growth': 596, 'laid': 745, 'set': 1217, 'nitpick': 916, 'user': 1496, 'identical': 672, 'holding': 649, 'lean': 759, 'forward': 537, 'happend': 611, 'prior': 1065, 'remembering': 1136, 'wasi': 1529, 'kept': 729, 'moods': 872, 'became': 122, 'tool': 1436, 'documenting': 383, 'recording': 1122, 'diapering': 361, 'session': 1216, 'wayside': 1538, 'finding': 504, 'flexible': 517, 'lifestyle': 773, 'nursing': 931, 'move': 881, 'peruse': 1007, 'secondary': 1202, 'account': 24, 'talking': 1371, 'personas': 1006, 'christmas': 235, 'february': 485, 'restless': 1150, 'blending': 147, '8217': 15, 'religiously': 1132, 'stored': 1326, 'defiantly': 344, 'tricky': 1461, 'print': 1063, 'light': 776, 'grey': 592, 'bottles': 157, 'meds': 843, 'challenge': 213, 'detailed': 354, 'slots': 1258, 'prefer': 1052, 'chooses': 233, 'sleeps': 1253, 'bath': 118, 'doc': 381, 'appt': 76, 'thank': 1391, 'stayed': 1315, 'suitcase': 1346, 'loose': 800, 'output': 970, 'crucial': 322, 'iphone': 708, 'proved': 1078, 'annoying': 64, 'walked': 1518, 'scrap': 1193, 'inaccurate': 682, 'update': 1487, 'continued': 295, 'control': 297, 'add': 32, 'rely': 1133, '3mo': 10, 'chair': 212, 'jotting': 722, 'scheduling': 1192, 'twins': 1473, 'four': 540, 'givers': 570, 'inside': 694, 'feed': 487, 'intervals': 702, '30': 8, 'ribbon': 1160, 'movable': 880, 'tab': 1362, 'advertised': 38, 'keeps': 728, 'sane': 1180, 'esp': 447, 'realistic': 1109, 'filling': 501, 'download': 393, 'embrace': 430, 'world': 1581, 'entering': 441, 'tabulate': 1365, 'charts': 219, 'review': 1156, 'myself': 889, 'daycare': 338, 'workers': 1578, 'offended': 939, 'explaining': 470, 'usefulness': 1494, 'greatest': 591, 'inventions': 706, 'relying': 1134, 'learning': 761, 'overwhelming': 974, 'history': 646, 'reactions': 1105, 'coupled': 310, 'itzbeen': 717, 'pocket': 1035, 'timer': 1426, 'necessity': 900, 'lost': 802, 'mind': 857, 'takers': 1368, 'picking': 1013, 'run': 1174, 'anyone': 69, 'expecting': 465, 'items': 715, 'intake': 698, 'organize': 962, 'lifesaver': 772, 'exhausted': 462, 'sides': 1236, 'plenty': 1032, 'handing': 605, 'parent': 988, 'whoever': 1558, 'ran': 1100, 'wishing': 1567, 'researching': 1145, 'settled': 1218, 'regret': 1129, 'laugh': 752, 'progression': 1073, 'toward': 1443, 'schedules': 1191, 'count': 308, 'introduced': 705, 'foods': 523, 'offered': 940, 'signs': 1237, 'intolerance': 704, 'arrival': 83, 'seasoned': 1200, 'therapist': 1398, 'brings': 175, 'ladder': 744, 'fire': 509, 'truck': 1465, 'bored': 153, 'roll': 1165, 'nesting': 908, 'improved': 678, 'cars': 200, 'random': 1101, 'teach': 1379}\n",
      "\n",
      "Word counts for the review:\n",
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 2: Define the new review\n",
    "review = \"This crib was amazing\"\n",
    "\n",
    "# Step 3: Create a CountVectorizer and name it 'counter'\n",
    "counter = CountVectorizer()\n",
    "\n",
    "# Step 4: Fit the vectorizer on the combined list of negative and positive reviews\n",
    "# This teaches the vectorizer the full vocabulary across all training data.\n",
    "counter.fit(neg_list + pos_list)\n",
    "\n",
    "# Step 5: Print the learned vocabulary\n",
    "# The output is a dictionary mapping each word to an index.\n",
    "print(\"Vocabulary learned by CountVectorizer:\")\n",
    "print(counter.vocabulary_)\n",
    "\n",
    "# Step 6: Transform the new review into a count vector\n",
    "# .transform() converts text into an array of word frequencies.\n",
    "# We wrap 'review' in a list because transform expects an iterable of strings.\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "# Step 7: Convert the sparse matrix to a readable dense array\n",
    "# The indices corresponding to the words in the review will have counts of 1.\n",
    "print(\"\\nWord counts for the review:\")\n",
    "print(review_counts.toarray())\n",
    "\n",
    "# Step 8: Transform the full training dataset\n",
    "# This converts all training reviews into count vectors that will be used for model training.\n",
    "training_counts = counter.transform(neg_list + pos_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d547d98",
   "metadata": {},
   "source": [
    "## Using scikit-learn\n",
    "\n",
    "Now that our data is formatted correctly, we can train a **Naive Bayes classifier** using **scikit-learn’s `MultinomialNB`** model.\n",
    "\n",
    "This classifier is designed for discrete data such as word counts, making it ideal for **text classification**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Training the Model\n",
    "\n",
    "The model is trained using the `.fit()` method.  \n",
    "This method takes two parameters:\n",
    "1. The **array of data points** — in this case, the matrix of word counts we created (`training_counts`).\n",
    "2. The **array of labels** — a list indicating whether each review is positive or negative.\n",
    "\n",
    "During training, the classifier learns the statistical relationship between word occurrences and their associated class labels.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Making Predictions\n",
    "\n",
    "Once trained, we can use the `.predict()` method to classify new data.  \n",
    "It takes a list of data points (such as the transformed vector of a new review) and returns the **predicted labels**.\n",
    "\n",
    "For example:\n",
    "- A review containing positive words like “great” or “amazing” would likely be classified as **positive**.\n",
    "- A review containing negative words like “terrible” or “broken” would likely be classified as **negative**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Checking Probabilities\n",
    "\n",
    "If we want more than just the final prediction, we can use the `.predict_proba()` method.  \n",
    "This method returns the **probability** of each possible label, allowing us to see how confident the model is in its prediction.\n",
    "\n",
    "Example output for a single review might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98cc215e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes order: [0 1]\n",
      "\n",
      "Review: 'this crib was great amazing and wonderful'\n",
      " Predicted label: 0 (negative)\n",
      " Probabilities -> negative: 0.699059, positive: 0.300941\n",
      "------------------------------------------------------------\n",
      "Review: 'this crib was terrible and broken'\n",
      " Predicted label: 0 (negative)\n",
      " Probabilities -> negative: 0.831156, positive: 0.168844\n",
      "------------------------------------------------------------\n",
      "Review: 'okay quality but not impressive'\n",
      " Predicted label: 0 (negative)\n",
      " Probabilities -> negative: 0.974334, positive: 0.025666\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1: Build labels aligned with the order used to stack texts\n",
    "#   - 0 for each negative review, 1 for each positive review\n",
    "training_texts = neg_list + pos_list\n",
    "training_labels = np.array([0] * len(neg_list) + [1] * len(pos_list))\n",
    "\n",
    "# Step 1.2: Vectorize texts into count features\n",
    "counter = CountVectorizer()\n",
    "training_counts = counter.fit_transform(training_texts)\n",
    "\n",
    "# Step 1.3: Initialize and fit the Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_counts, training_labels)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.8.2  Step 2: Making Predictions\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Step 2.1: Create some new reviews to classify (feel free to edit these)\n",
    "new_reviews = [\n",
    "    \"this crib was great amazing and wonderful\",   # likely positive\n",
    "    \"this crib was terrible and broken\",           # likely negative\n",
    "    \"okay quality but not impressive\"              # ambiguous/neutral-ish\n",
    "]\n",
    "\n",
    "# Step 2.2: Transform new reviews into count vectors using the SAME fitted vectorizer\n",
    "new_counts = counter.transform(new_reviews)\n",
    "\n",
    "# Step 2.3: Predict the class labels (0=negative, 1=positive)\n",
    "pred_labels = classifier.predict(new_counts)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Step 3: Checking Probabilities\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Step 3.1: Get class probabilities for each review\n",
    "#   - columns correspond to classes in classifier.classes_ (sorted)\n",
    "pred_proba = classifier.predict_proba(new_counts)\n",
    "\n",
    "# Step 3.2: Pretty-print results\n",
    "label_names = {0: \"negative\", 1: \"positive\"}\n",
    "print(\"Classes order:\", classifier.classes_)  # e.g., [0 1]\n",
    "print()\n",
    "\n",
    "for text, label, proba in zip(new_reviews, pred_labels, pred_proba):\n",
    "    neg_p, pos_p = proba[0], proba[1]  # assuming classes_ = [0, 1]\n",
    "    print(f\"Review: {text!r}\")\n",
    "    print(f\" Predicted label: {label} ({label_names[label]})\")\n",
    "    print(f\" Probabilities -> negative: {neg_p:.6f}, positive: {pos_p:.6f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Example of expected output format for a strongly positive review might resemble:\n",
    "# [[0.049777 0.950223]]  (values will differ with this toy dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf9bbf",
   "metadata": {},
   "source": [
    "# Final Remarks — What You Built, Limits, and Next Steps\n",
    "\n",
    "You implemented the full Naive Bayes workflow for text: token counts, priors, smoothed likelihoods, and posterior comparison. The model is fast, simple, and effective on many sparse text tasks.\n",
    "\n",
    "### Practical tips\n",
    "- Ensure \\(X.shape[0] = |y|\\) and that label order matches how texts were stacked.\n",
    "- Prefer log-probabilities for numerical stability.\n",
    "- Use proper train/validation/test splits; evaluate with accuracy, precision/recall, F1, and confusion matrices.\n",
    "\n",
    "### Limitations\n",
    "- Independence assumption ignores word order and context.\n",
    "- Performance can drop with domain shift or heavy misspellings; smoothing helps but is limited.\n",
    "\n",
    "### Useful extensions\n",
    "- Replace raw counts with TF-IDF features.\n",
    "- Add n-grams (bigrams/trigrams) to capture short phrases.\n",
    "- Try alternative linear models (e.g., Logistic Regression, Linear SVM) on the same features.\n",
    "- Apply light preprocessing: lowercasing, punctuation/number handling, and task-dependent stopword treatment.\n",
    "\n",
    "These steps give you a strong, reproducible baseline for many text-classification problems and a clear path for iterative improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac64c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
