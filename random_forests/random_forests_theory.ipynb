{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78eb1b2f",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size: 28px;\"><b>Random Forests</b></p>\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Random forests aren’t just for classification—they also work well for **regression** when the target is continuous (e.g., a numeric `price`). The model builds many decision trees on **bootstrapped** samples of the data and, at each split, considers a **random subset of features**. For regression, each tree outputs a number and the forest prediction is the **average** of those outputs, which reduces variance and typically generalizes better than a single tree.\n",
    "\n",
    "In scikit-learn, `RandomForestRegressor` follows the familiar workflow: `.fit()`, `.predict()`, and `.score()`. By default, `.score()` reports the **\\(R^2\\)** (coefficient of determination):\n",
    "\n",
    "$$\n",
    "R^2 \\;=\\; 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "Alongside \\(R^2\\), it’s useful to track **MAE** and **RMSE** to understand typical error magnitudes:\n",
    "\n",
    "- \\( \\text{MAE} = \\frac{1}{n}\\sum |y_i - \\hat{y}_i| \\)\n",
    "- \\( \\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2} \\)\n",
    "\n",
    "Key knobs include `n_estimators`, `max_depth`, `min_samples_leaf`, and `max_features`. With one-hot encoded categoricals, random forests often perform strongly out of the box with modest tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112b0dd",
   "metadata": {},
   "source": [
    "# Basics of a Random Forest\n",
    "\n",
    "A **random forest** is an ensemble machine learning algorithm that combines multiple decision trees to improve prediction accuracy and reduce overfitting.\n",
    "\n",
    "While **decision trees** are powerful supervised learning models, they can easily overfit the training data. Techniques such as pruning can reduce this issue but may not always be sufficient. Random forests solve this problem by aggregating the results of many trees.\n",
    "\n",
    "When classifying a new data point:\n",
    "\n",
    "1. The point is passed to each tree in the forest.  \n",
    "2. Each tree provides its own classification.  \n",
    "3. The forest outputs the **majority vote** — the most common classification among all trees.\n",
    "\n",
    "This voting process reduces the influence of overfitted trees, resulting in a more generalized and reliable model.\n",
    "\n",
    "*Prerequisite:* A solid understanding of decision trees is recommended before diving into random forests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8202f782",
   "metadata": {},
   "source": [
    "# Bootstrapping\n",
    "\n",
    "When building a random forest, each decision tree must be different to ensure variety and robustness. However, a single decision tree algorithm is **deterministic** — it produces the same structure given the same dataset. To overcome this, random forests use a technique called **bagging** (short for *bootstrap aggregating*), which relies on a statistical method known as **bootstrapping**.\n",
    "\n",
    "Bootstrapping is a type of **sampling with replacement**, meaning that each time we draw a data point from the dataset, we put it back before drawing again. This allows the same observation to be selected multiple times in the same sample.\n",
    "\n",
    "**How Bootstrapping Works**\n",
    "\n",
    "1. Suppose our training dataset has 1000 rows.  \n",
    "2. For each tree in the forest, we randomly pick a subset (for example, 100 rows) **with replacement** to build that tree.  \n",
    "3. Because we replace each row after drawing it, the same row can appear more than once, while others may not appear at all.  \n",
    "\n",
    "As a result, each decision tree is trained on a different bootstrapped dataset, which introduces diversity and helps reduce overfitting when the trees are combined into a random forest.\n",
    "\n",
    "**Intuitive Example**\n",
    "\n",
    "Imagine placing all 1000 rows of your dataset into a bag. You reach in, grab one row at random, note it down, and then put it back before drawing again. Repeating this process ensures that some rows are picked multiple times, while others might not be picked at all — yet you still end up with a valid dataset of the same size.\n",
    " \n",
    "**Example Dataset**\n",
    "\n",
    "We’ll illustrate this with a **car evaluation dataset**, which includes features like safety, buying price, and passenger capacity. The goal is to use bootstrapping to estimate the **average safety rating** (`low`, `med`, or `high`) across multiple resampled datasets.\n",
    "\n",
    "| Variable   | Description |\n",
    "|-------------|-------------|\n",
    "| `safety`    | Estimated safety of the car (`low`, `med`, or `high`) |\n",
    "| `buying`    | Buying price |\n",
    "| `maint`     | Maintenance cost |\n",
    "| `doors`     | Number of doors |\n",
    "| `persons`   | Passenger capacity |\n",
    "| `lug_boot`  | Size of luggage boot |\n",
    "| `accep`     | Evaluation level (`unacceptable`, `acceptable`, `good`, `very good`) |\n",
    "\n",
    "Bootstrapping is the foundation of random forests — it ensures that each tree sees a slightly different view of the data, making the ensemble stronger, more stable, and less prone to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0299ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "# pandas and numpy for data manipulation\n",
    "# matplotlib and seaborn for visualization\n",
    "# sklearn for creating and training decision trees / random forests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3addecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the car evaluation dataset and name the columns\n",
    "df = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data',\n",
    "    names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep']\n",
    ")\n",
    "\n",
    "# Step 3: Convert the target variable `accep` into a binary indicator\n",
    "# Here, 'unacc' is converted to 0 (not acceptable), all others become 1 (acceptable)\n",
    "df['accep'] = ~(df['accep'] == 'unacc')\n",
    "\n",
    "# Step 4: Prepare the features (X) and target (y)\n",
    "# We use one-hot encoding for categorical variables to make them numeric\n",
    "X = pd.get_dummies(df.iloc[:, 0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "\n",
    "# Step 5: Split the dataset into training and testing sets (75% train, 25% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)\n",
    "\n",
    "# Step 6: Get the total number of rows for later use\n",
    "nrows = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edfe3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728\n",
      "Distribution of safety ratings in 1728 rows of data:\n",
      "safety\n",
      "low     0.333333\n",
      "med     0.333333\n",
      "high    0.333333\n",
      "Name: proportion, dtype: float64\n",
      "Distribution of safety ratings in bootstrapped sample data:\n",
      "safety\n",
      "med     0.345486\n",
      "high    0.329861\n",
      "low     0.324653\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# ✅ Checkpoint 1: Print dataset size and safety distribution\n",
    "# -------------------------------------------------------\n",
    "# Print the total number of rows and the distribution of safety ratings\n",
    "print(nrows)\n",
    "print(f'Distribution of safety ratings in {nrows} rows of data:')\n",
    "print(df.safety.value_counts(normalize=True))\n",
    "# Observation: Each safety class (\"low\", \"med\", \"high\") occurs equally — around 0.333 each.\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# ✅ Checkpoint 2: Create a single bootstrapped sample\n",
    "# -------------------------------------------------------\n",
    "# Bootstrapping = sampling with replacement\n",
    "# Here we randomly pick nrows samples, allowing duplicates\n",
    "boot_sample = df.sample(nrows, replace=True)\n",
    "\n",
    "# Print the new distribution of safety ratings\n",
    "print(f'Distribution of safety ratings in bootstrapped sample data:')\n",
    "print(boot_sample.safety.value_counts(normalize=True))\n",
    "# Observation: The proportions now vary — they are no longer perfectly equal.\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# ✅ Checkpoint 3: Repeat bootstrapping 1000 times\n",
    "# -------------------------------------------------------\n",
    "# We’ll repeat the sampling process 1000 times and record\n",
    "# the proportion of \"low\" safety ratings in each bootstrapped sample\n",
    "low_perc = []\n",
    "for i in range(1000):\n",
    "    boot_sample = df.sample(nrows, replace=True)\n",
    "    low_perc.append(boot_sample.safety.value_counts(normalize=True)[\"low\"])\n",
    "\n",
    "# Convert list to numpy array for easy calculations\n",
    "low_perc = np.array(low_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6cb25",
   "metadata": {},
   "source": [
    "**A digression Calculating the proportion of rows where `safety == \"low\"**`\n",
    "\n",
    "There are several ways to compute the proportion of rows in a DataFrame where the column `safety` equals `\"low\"`.\n",
    "\n",
    "✅ Correct methods:\n",
    "\n",
    "1. Using `value_counts()` with normalization:\n",
    "\n",
    "`boot_sample.safety.value_counts(normalize=True)[\"low\"]`\n",
    "\n",
    "→ Returns the relative frequency (proportion) of `\"low\"` values directly.\n",
    "\n",
    "2. Using boolean comparison and division:\n",
    "\n",
    "`(boot_sample.safety == \"low\").sum() / boot_sample.safety.count()`\n",
    "\n",
    "→ The boolean comparison creates a Series of `True`/`False` values, and `.sum()` counts the number of `True` values.\n",
    "\n",
    "3. Using the mean of the boolean Series:\n",
    "\n",
    "`(boot_sample.safety == \"low\").mean()`\n",
    "\n",
    "→ Since `True` is treated as `1` and `False` as `0`, the mean gives the fraction of `True` values.\n",
    "\n",
    "❌ Incorrect method:\n",
    "\n",
    "`(boot_sample.safety == \"low\").count() / boot_sample.safety.count()`\n",
    "\n",
    "This always returns `1.0` because `.count()` counts all non-missing entries, **not only those equal to `\"low\"`**.\n",
    "\n",
    "✅ Recommended:\n",
    "The cleanest and most readable approach is:\n",
    "\n",
    "`(boot_sample.safety == \"low\").mean()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8869f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of 'low' safety proportions: 0.3333\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# ✅ Checkpoint 4: Plot histogram of low safety proportions\n",
    "# -------------------------------------------------------\n",
    "mean_lp = np.mean(low_perc)\n",
    "print(f\"Mean of 'low' safety proportions: {mean_lp:.4f}\")\n",
    "\n",
    "# A diagramm with the distribution pf the percentage values for \"low\", they t3end to the mean value of the original Datafgrame\n",
    "plt.hist(low_perc, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Bootstrapped Low Safety Percentage Distribution')\n",
    "plt.xlabel('Low Safety Percentage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# Observation: The histogram shows a bell-shaped distribution\n",
    "# centered around the true mean proportion (~0.333)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f1176d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average low percentage: 0.3333\n",
      "95% Confidence Interval for low percentage: (0.3125, 0.3553)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# ✅ Checkpoint 5: Calculate 95% Confidence Interval\n",
    "# -------------------------------------------------------\n",
    "# Sort the percentages and extract the 2.5th and 97.5th percentiles\n",
    "low_perc.sort()\n",
    "lower_bound = low_perc[25].round(4)\n",
    "upper_bound = low_perc[975].round(4)\n",
    "\n",
    "print(f'Average low percentage: {np.mean(low_perc).round(4)}')\n",
    "print(f'95% Confidence Interval for low percentage: ({lower_bound}, {upper_bound})')\n",
    "# Interpretation: We are 95% confident that the true proportion\n",
    "# of \"low\" safety ratings lies within this interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3ea08",
   "metadata": {},
   "source": [
    "Bootstrapping: Why Sampling *With Replacement* Matters\n",
    "\n",
    "Bootstrapping introduces randomness into each decision tree by sampling **with replacement** from the training data.  \n",
    "Although this can over-represent some observations, it’s a deliberate design choice that improves the overall robustness of the random forest.\n",
    "\n",
    "Key Reasons for Using Bootstrapping  \n",
    "- **Simulates Sampling Variability:** Mimics drawing multiple datasets from the same population, allowing better estimation of variability and reducing overfitting.  \n",
    "- **Increases Tree Diversity:** Each tree sees a different view of the data, leading to uncorrelated models whose errors average out.  \n",
    "- **Statistical Foundation:** Bootstrapping’s “resample-with-replacement” design ensures each sample behaves like an independent dataset.  \n",
    "\n",
    "Why Not Sample Without Replacement?  \n",
    "- Produces less diverse trees, reducing the benefit of ensemble averaging.  \n",
    "- Samples are more similar to the original dataset (lower variance, higher bias).  \n",
    "- Fewer unique dataset combinations, limiting randomness across trees.  \n",
    "\n",
    "Trade-Off Summary  \n",
    "| Method | Pros | Cons |\n",
    "|--------|------|------|\n",
    "| **With Replacement (Bootstrapping)** | High diversity, strong ensemble effect, well-established method | Some rows may appear multiple times |\n",
    "| **Without Replacement (Subsampling)** | Simpler, no duplicates | Lower diversity, weaker ensemble performance |\n",
    "\n",
    "Takeaway  \n",
    "Bootstrapping intentionally adds variability by resampling with replacement, creating more diverse and independent trees that, when combined, make the random forest more stable and accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74805f23",
   "metadata": {},
   "source": [
    "**Interpretation of the Bootstrapping Result**\n",
    "\n",
    "If the proportion of \"low\" safety values in the bootstrapped sample is close to the original dataset’s proportion, this indicates that:\n",
    "\n",
    "Although bootstrapping resamples the data with replacement,\n",
    "\n",
    "The overall distribution of categories (e.g., \"low\", \"med\", \"high\") remains roughly consistent with the original dataset.\n",
    "\n",
    "In essence, bootstrapping introduces random variation while preserving the underlying statistical structure of the data.\n",
    "\n",
    "Each bootstrapped sample is a slightly different version of the original dataset, yet the relative frequency of \"low\" safety ratings tends to remain similar to the original.\n",
    "\n",
    "This consistency ensures that ensemble methods like Random Forests can combine results from many bootstrapped trees without losing the core patterns of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3123c6",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Bagging, short for Bootstrap Aggregating, is the process by which random forests create diverse decision trees and then combine their predictions to improve model performance.\n",
    "\n",
    "The idea begins with bootstrapping — training each decision tree on a different subset of the data sampled with replacement. Once multiple trees have been trained, their results are aggregated to form the final prediction.\n",
    "\n",
    "How Bagging Works:\n",
    "\n",
    "1. Bootstrap Sampling: Each tree is trained on a random subset of the training data, created through bootstrapping.  \n",
    "2. Independent Model Training: Every tree is grown independently from its own bootstrapped sample.  \n",
    "3. Aggregation of Results:  \n",
    "   - For classification, the final prediction is determined by majority vote across all trees.  \n",
    "   - For regression, the final prediction is the average of all tree outputs.  \n",
    "\n",
    "This ensemble approach significantly reduces variance and helps prevent overfitting — one of the main weaknesses of a single decision tree.\n",
    "\n",
    "Example Dataset:\n",
    "\n",
    "We’ll apply bagging to the same car evaluation dataset used previously, which contains the following features:\n",
    "\n",
    "| Feature  | Description |\n",
    "|-----------|-------------|\n",
    "| `buying`  | Buying price (`vhigh`, `high`, `med`, `low`) |\n",
    "| `maint`   | Maintenance cost (`vhigh`, `high`, `med`, `low`) |\n",
    "| `doors`   | Number of doors (`2`, `3`, `4`, `5more`) |\n",
    "| `persons` | Passenger capacity (`2`, `4`, `more`) |\n",
    "| `lugboot` | Size of trunk (`small`, `med`, `big`) |\n",
    "| `safety`  | Safety rating (`low`, `med`, `high`) |\n",
    "\n",
    "The target variable is `accep`, which indicates whether a car is acceptable (True) or unacceptable (False).\n",
    "\n",
    "By combining multiple trees through bagging, a random forest achieves higher stability, reduces overfitting, and generally improves predictive accuracy compared to individual decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febb5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "724c1e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 1 — DT@full-train accuracy: 0.8588\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# ✅ Checkpoint 1\n",
    "# Train a decision tree with max_depth=5 and evaluate accuracy on test data.\n",
    "# -------------------------------------------------------\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred = dt.predict(x_test)\n",
    "base_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Checkpoint 1 — DT@full-train accuracy: {base_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9cdc1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 2 — DT@bootstrap(rs=0) accuracy: 0.8912\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# ✅ Checkpoint 2\n",
    "# Use a provided bootstrapped set of indices (random_state=0) to fit another tree.\n",
    "# Evaluate accuracy on the test set for this new classifier.\n",
    "# -------------------------------------------------------\n",
    "# This gives us back the indices of the new sampled distribution\n",
    "ids = x_train.sample(len(x_train), replace=True, random_state=0).index\n",
    "dt_boot = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "# the list of indices get applied to the x_train and y_train to assure \n",
    "# that the right combination of x and y are used for the training\n",
    "dt_boot.fit(x_train.loc[ids], y_train.loc[ids])\n",
    "boot_pred = dt_boot.predict(x_test)\n",
    "boot_acc = accuracy_score(y_test, boot_pred)\n",
    "print(f\"Checkpoint 2 — DT@bootstrap(rs=0) accuracy: {boot_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f024dcf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False False False  True False False  True  True False False\n",
      "  True False False  True  True False  True False  True  True  True False\n",
      " False False False False  True False  True  True False  True  True False\n",
      "  True False False False False False False False  True False False  True\n",
      " False False False  True False False False False False False False  True\n",
      "  True  True False  True  True  True False False False False False  True\n",
      "  True False  True  True False  True  True False  True  True False False\n",
      "  True False False  True False False False  True  True False False  True\n",
      " False  True  True  True False  True  True False  True  True  True  True\n",
      " False False False False  True  True False  True  True False False False\n",
      " False False  True False False False False  True  True False False False\n",
      "  True False False False False False  True False False False False False\n",
      " False False  True False False  True  True  True False False False False\n",
      " False False False False  True False False False  True False False False\n",
      " False False False False False False  True False False False False False\n",
      "  True  True False False  True False  True False False False False False\n",
      " False  True False False False  True False  True False  True  True False\n",
      " False False False False False False False  True False False False False\n",
      " False  True False False False False False  True False False False  True\n",
      " False False False False  True False  True False  True  True False False\n",
      " False False False False  True False  True False False False False False\n",
      "  True  True  True  True  True False False False  True False  True  True\n",
      " False False  True False  True False False False False False  True False\n",
      " False False  True False False False  True False False  True False  True\n",
      "  True False False False False False False False False False False  True\n",
      " False  True  True  True  True False  True False  True False  True  True\n",
      " False False  True False  True False False  True False False  True False\n",
      "  True  True  True False False False  True False  True False  True False\n",
      " False  True  True False False  True False  True False  True False False\n",
      "  True False  True False False  True  True False False  True False  True\n",
      " False  True False False  True  True  True  True False  True False False\n",
      " False  True False False False  True  True False  True False  True False\n",
      " False False  True False False False False False False  True False False\n",
      "  True False False  True False  True False False  True False False False\n",
      "  True  True False False False  True False False  True False False  True\n",
      " False  True  True False False False  True False False  True False  True]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# ✅ Checkpoint 3\n",
    "# Build 10 different bootstrapped trees in a loop.\n",
    "# Save each test-set prediction (y_pred) into 'preds'.\n",
    "# Take the average (bagging) and store as 'ba_pred' (will be floats in [0,1]).\n",
    "# -------------------------------------------------------\n",
    "preds = []\n",
    "for i in range(10):\n",
    "    # New bootstrap sample with a different seed each iteration\n",
    "    ids_i = x_train.sample(len(x_train), replace=True, random_state=i).index\n",
    "    tree_i = DecisionTreeClassifier(max_depth=5, random_state=i)\n",
    "    tree_i.fit(x_train.loc[ids_i], y_train.loc[ids_i])\n",
    "    # This appends a list of True and False to pred\n",
    "    preds.append(tree_i.predict(x_test))  # 0/1 array\n",
    "\n",
    "preds = np.array(preds)             # shape: (10, n_test)\n",
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d65218b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 432)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3ea0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns boolean into numbers, how? True is a 1 and False is a 0, so the mean calculates the average column-wise\n",
    "# being preds.shape == (10, 432), it returns an array oflen 432, \n",
    "# with the average of the predictions (True/False or 1/0) for the 10 trees\n",
    "ba_pred = preds.mean(axis=0)        # averaged predictions (floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "238ea467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 4 — Bagged(10) accuracy: 0.9097\n",
      "\n",
      "Summary:\n",
      "  Single DT (full train):       0.8588\n",
      "  Single DT (bootstrap rs=0):   0.8912\n",
      "  Bagged DTs (10 models):       0.9097\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# ✅ Checkpoint 4\n",
    "# Compute accuracy of the bagged predictions.\n",
    "# Note: 'ba_pred' is averaged (floats), so threshold at 0.5 to get binary labels.\n",
    "# -------------------------------------------------------\n",
    "# this turns once more the numbers int boolean, \n",
    "# giving back an array of True if the value is >= 0.5 (half of the tree returned True > makority vote)\n",
    "# otherwise giving False\n",
    "ba_accuracy = accuracy_score(ba_pred>=0.5, y_test)\n",
    "print(f\"Checkpoint 4 — Bagged(10) accuracy: {ba_accuracy:.4f}\")\n",
    "\n",
    "# (Optional) Quick summary\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  Single DT (full train):       {base_acc:.4f}\")\n",
    "print(f\"  Single DT (bootstrap rs=0):   {boot_acc:.4f}\")\n",
    "print(f\"  Bagged DTs (10 models):       {ba_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25693e",
   "metadata": {},
   "source": [
    "**What the code is doing (step-by-step)**\n",
    "\n",
    "preds construction\n",
    "\n",
    "Each loop trains a DecisionTreeClassifier on a bootstrapped version of x_train, y_train.\n",
    "\n",
    "dt.predict(x_test) returns a vector of booleans (True/False) because your y is boolean.\n",
    "\n",
    "preds becomes a list of 10 such vectors → shape (10, len(x_test)), e.g. (10, 432).\n",
    "\n",
    "Averaging predictions\n",
    "\n",
    "np.array(preds) converts to a 2D boolean array; in NumPy, True→1, False→0.\n",
    "\n",
    "mean(0) computes the per-sample average across trees, i.e. the fraction of trees voting True for each test point.\n",
    "\n",
    "ba_pred ∈ [0,1] for each sample (e.g., 0.7 means 7/10 trees predicted True).\n",
    "\n",
    "Majority vote via thresholding\n",
    "\n",
    "(ba_pred >= 0.5) converts those fractions back to booleans (majority voting).\n",
    "\n",
    "So you’re not comparing booleans to numbers in accuracy_score; you’ve converted the numbers to booleans first.\n",
    "\n",
    "Accuracy computation\n",
    "\n",
    "accuracy_score(y_true, y_pred) compares two boolean arrays element-wise and reports the fraction equal.\n",
    "\n",
    "In your call you wrote accuracy_score(ba_pred>=0.5, y_test). It still works (symmetry), but the conventional order is accuracy_score(y_test, ba_pred>=0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1e3b1",
   "metadata": {},
   "source": [
    "# Random Feature Selection\n",
    "\n",
    "In addition to bootstrapping samples from the dataset, we can introduce further variability by randomly selecting which features are used to train each tree.\n",
    "\n",
    "For the car dataset, the original features are:\n",
    "\n",
    "- Price of the car: “vhigh”, “high”, “med”, or “low”  \n",
    "- Maintenance cost: “vhigh”, “high”, “med”, or “low”  \n",
    "- Number of doors: “2”, “3”, “4”, or “5more”  \n",
    "- Capacity (number of people): “2”, “4”, or “more”  \n",
    "- Trunk size: “small”, “med”, or “big”  \n",
    "- Safety rating: “low”, “med”, or “high”\n",
    "\n",
    "The target variable, `accep`, represents whether a car is acceptable (`True` or `False`).  \n",
    "After dummy encoding the categorical features, the dataset contains 15 total features for training (`x_train`) and testing (`x_test`).\n",
    "\n",
    "When using a single decision tree, all available features are considered at each split, and the best one is chosen based on the highest information gain.  \n",
    "However, in ensemble methods like Random Forests, **randomly selecting a subset of fea**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed8ef93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DT on test set (full feature set):\n",
      "0.9467592592592593\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Random Feature Selection with Aggregated Predictions (Ensemble)\n",
    "# ------------------------------------------------------------\n",
    "# Goal:\n",
    "# Embed the step-by-step instructions directly into executable code,\n",
    "# with clear, numbered comments aligned to the checkpoints.\n",
    "\n",
    "# Train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, test_size=0.25\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Baseline Decision Tree (All Features)\n",
    "# ----------------------------\n",
    "# Train a baseline tree using the full feature set and evaluate on the test set\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train, y_train)\n",
    "accuracy_dt = dt.score(x_test, y_test)\n",
    "print(\"Accuracy of DT on test set (full feature set):\")\n",
    "print(accuracy_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4afa6522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DT on test set (random feature sample):\n",
      "0.7291666666666666\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Checkpoint 1\n",
    "# ============================================================\n",
    "# Step 1.1: Create a random sample of 10 features from the 15 total features\n",
    "rand_features = np.random.choice(x_train.columns, 10)\n",
    "\n",
    "# Step 1.2: Define a new decision tree classifier (no pre-selected hyperparameters)\n",
    "dt2 = DecisionTreeClassifier()\n",
    "\n",
    "# Step 1.3: Train dt2 ONLY on the selected random feature subset\n",
    "dt2.fit(x_train[rand_features], y_train)\n",
    "\n",
    "# Step 1.4: Evaluate on the test set using ONLY those same features\n",
    "accuracy_dt2 = dt2.score(x_test[rand_features], y_test)\n",
    "print(\"Accuracy of DT on test set (random feature sample):\")\n",
    "print(accuracy_dt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc74cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Checkpoint 2\n",
    "# ============================================================\n",
    "# Step 2.1: Initialize a container for predictions from 10 different random-feature trees\n",
    "predictions = []\n",
    "\n",
    "# Step 2.2: Loop ten times; each iteration:\n",
    "#   - Selects 10 random features\n",
    "#   - Fits dt2 on the corresponding training subset\n",
    "#   - Appends predictions on the corresponding test subset\n",
    "for _ in range(10):\n",
    "    rand_features = np.random.choice(x_train.columns, 10)\n",
    "    dt2.fit(x_train[rand_features], y_train)\n",
    "    preds = dt2.predict(x_test[rand_features])\n",
    "    predictions.append(preds)\n",
    "\n",
    "# Convert list of arrays into a 2D NumPy array of shape (10, n_samples)\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c64e2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of aggregated predictions from 10 random-feature trees:\n",
      "0.7175925925925926\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Checkpoint 3\n",
    "# ============================================================\n",
    "# Step 3.1: Compute the mean prediction per sample across the 10 classifiers.\n",
    "#           Since it's binary (False/True → 0/1), this acts like a probability.\n",
    "prob_predictions = predictions.mean(axis=0)\n",
    "\n",
    "# Step 3.2: Aggregate to a final class prediction using majority vote (> 0.5 → True)\n",
    "agg_predictions = prob_predictions > 0.5\n",
    "\n",
    "# Step 3.3: Compute accuracy of the aggregated predictions\n",
    "# NOTE: accuracy_score expects (y_true, y_pred)\n",
    "agg_accuracy = accuracy_score(y_test, agg_predictions)\n",
    "\n",
    "print(\"Accuracy of aggregated predictions from 10 random-feature trees:\")\n",
    "print(agg_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266e94a",
   "metadata": {},
   "source": [
    "**Why the accuracy drops when using random feature subsets**\n",
    "\n",
    "Your experiment tests three setups:\n",
    "1) Single tree with all features\n",
    "2) Single tree with a random subset of features\n",
    "3) An ensemble of 10 trees, each trained on a random subset of features\n",
    "\n",
    "Why accuracy often decreases in this demo\n",
    "\n",
    "- You weakened each tree without enough pay-off  \n",
    "  Limiting features reduces each tree’s strength. Ensembles win when many *reasonably strong* but *less correlated* trees are combined. With only 10 trees, the strength loss can outweigh the benefit.\n",
    "\n",
    "- Randomness is at the tree level, not the split level  \n",
    "  True Random Forests sample a subset of features at *every split*, creating much stronger decorrelation than picking 10 columns once per tree.\n",
    "\n",
    "- No bootstrapping of rows  \n",
    "  Random Forests also sample rows with replacement (bagging). Only randomizing features gives less diversity and weaker variance reduction.\n",
    "\n",
    "- Hard votes instead of probability averaging  \n",
    "  Averaging `predict` (hard 0/1) and thresholding at 0.5 discards uncertainty. Averaging `predict_proba` and then thresholding is usually better.\n",
    "\n",
    "- Single train/test split variance  \n",
    "  A plain tree can “luck out” on a particular split. Small datasets and a single split can mask the true generalization effect—use cross-validation.\n",
    "\n",
    "- Threshold and class imbalance  \n",
    "  If classes are imbalanced, a 0.5 threshold may be suboptimal. Metrics like AUC, F1, or balanced accuracy can give a fairer picture.\n",
    "\n",
    "What’s missing compared to a proper Random Forest\n",
    "\n",
    "- Feature subsampling at every split (`max_features`)\n",
    "- Row bootstrapping (`bootstrap=True`)\n",
    "- Many trees (typically 100–1000+)\n",
    "- Probability averaging (or majority vote across many trees)\n",
    "- Regularized trees (e.g., `max_depth`, `min_samples_leaf`) to reduce overfitting\n",
    "\n",
    "How to make the ensemble actually help\n",
    "\n",
    "1) Use a proper Random Forest\n",
    "\n",
    "2) If keeping your custom loop, average probabilities and add bootstrapping\n",
    "\n",
    "3) Evaluate robustly\n",
    "\n",
    "- Use stratified k-fold cross-validation to reduce split-specific noise.\n",
    "\n",
    "- Consider balanced accuracy, F1, or AUC, and tune the decision threshold if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d49317a",
   "metadata": {},
   "source": [
    "# Bagging in scikit-learn\n",
    "\n",
    "Bagging (“bootstrap aggregating”) combines two ideas we explored: training base learners on bootstrapped samples and (optionally) using random subsets of features. Doing both increases diversity among the learners and typically improves generalization by reducing variance while keeping bias roughly similar.\n",
    "\n",
    "In scikit-learn, this is implemented with `BaggingClassifier`. You provide:\n",
    "- the base model to bag (e.g., a `DecisionTreeClassifier(max_depth=5)`),\n",
    "- how many models to train (`n_estimators`),\n",
    "- how many samples/features each model sees (`max_samples`, `max_features`),\n",
    "- whether to sample rows/features with replacement (`bootstrap`, `bootstrap_features`).\n",
    "\n",
    "After instantiation, you use `.fit()`, `.predict()`, and `.score()` as usual.\n",
    "\n",
    "Example (decision tree as base estimator, depth 5):\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "base_tree = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "\n",
    "bag = BaggingClassifier(\n",
    "    base_estimator=base_tree,   # in newer versions: estimator=base_tree\n",
    "    n_estimators=100,           # number of bootstrapped trees\n",
    "    max_samples=1.0,            # fraction or count of rows per tree\n",
    "    max_features=1.0,           # fraction or count of features per tree\n",
    "    bootstrap=True,             # sample rows with replacement\n",
    "    bootstrap_features=False,   # (optional) sample features with replacement\n",
    "    n_jobs=-1,                  # use all cores\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "bag.fit(x_train, y_train)\n",
    "test_accuracy = bag.score(x_test, y_test)\n",
    "print(test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8741ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Bagging in scikit-learn — Checkpoints with Embedded Instructions\n",
    "# ------------------------------------------------------------\n",
    "# Goal: Implement bagging with different base estimators and feature sampling,\n",
    "# and report test accuracy for each configuration.\n",
    "\n",
    "# Train/test split (fixed random_state for reproducibility)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, test_size=0.25, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0be0a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "beec79a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of Bagged DT (10 estimators, all features):\n",
      "0.9189814814814815\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Checkpoint 1\n",
    "# ------------------------------------------------------------\n",
    "# Step 1.1: Instantiate BaggingClassifier with DecisionTreeClassifier(max_depth=5)\n",
    "#           Use 10 estimators; bootstrap rows (default True); use all features per estimator.\n",
    "bag_dt = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_depth=5, random_state=0),\n",
    "    n_estimators=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Step 1.2: Fit on training data\n",
    "bag_dt.fit(x_train, y_train)\n",
    "\n",
    "# Step 1.3: Evaluate on test data; store & print bag_accuracy\n",
    "bag_accuracy = bag_dt.score(x_test, y_test)\n",
    "print('Accuracy score of Bagged DT (10 estimators, all features):')\n",
    "print(bag_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cde5573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of Bagged DT (10 estimators, max_features=10):\n",
      "0.8773148148148148\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Checkpoint 2\n",
    "# ------------------------------------------------------------\n",
    "# Step 2.1: Instantiate BaggingClassifier with max_features=10\n",
    "#           (each base estimator gets a random subset of 10 features)\n",
    "bag_dt_10 = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_depth=5, random_state=0),\n",
    "    n_estimators=10,\n",
    "    max_features=10,        # draw 10 features (count) per estimator\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Step 2.2: Fit and evaluate; store & print bag_accuracy_10\n",
    "bag_dt_10.fit(x_train, y_train)\n",
    "bag_accuracy_10 = bag_dt_10.score(x_test, y_test)\n",
    "print('Accuracy score of Bagged DT (10 estimators, max_features=10):')\n",
    "print(bag_accuracy_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2045dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3af42f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of Bagged Logistic Regression (10 estimators, max_features=10):\n",
      "0.8611111111111112\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Checkpoint 3\n",
    "# ------------------------------------------------------------\n",
    "# Step 3.1: Change base estimator to LogisticRegression()\n",
    "#           Use solver suited for small/medium data; increase max_iter for convergence.\n",
    "bag_lr = BaggingClassifier(\n",
    "    LogisticRegression(\n",
    "        solver='liblinear',   # robust on small dense data\n",
    "        max_iter=1000,\n",
    "        random_state=0\n",
    "    ),\n",
    "    n_estimators=10,\n",
    "    max_features=10,\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Step 3.2: Fit and evaluate; store & print bag_accuracy_lr\n",
    "bag_lr.fit(x_train, y_train)\n",
    "bag_accuracy_lr = bag_lr.score(x_test, y_test)\n",
    "print('Accuracy score of Bagged Logistic Regression (10 estimators, max_features=10):')\n",
    "print(bag_accuracy_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2795712",
   "metadata": {},
   "source": [
    "# Random Forests: Train and Predict using scikit-learn\n",
    "\n",
    "Random forests combine decision trees using two kinds of randomness:\n",
    "- **Row sampling (bagging):** each tree is trained on a bootstrapped sample of the training data.\n",
    "- **Feature sampling at every split:** instead of choosing one random set of features once per tree, **each split** considers a fresh random subset of features.\n",
    "\n",
    "How split-level feature sampling works\n",
    "- First split: pick a random subset of features (e.g., price, doors, safety), choose the best splitter among them.\n",
    "- Next split(s): pick a **new** random subset (e.g., maint, doors, trunk), choose the best splitter, and continue until the tree is complete.\n",
    "- This per-split randomness reduces correlation among trees while keeping trees relatively strong.\n",
    "\n",
    "Choosing how many features per split\n",
    "- A common rule of thumb for classification: use about **\\(\\sqrt{d}\\)** features at each split, where \\(d\\) is the total number of features.\n",
    "- Example: if \\(d = 25\\), consider **5** features per split.\n",
    "- Small feature sets may limit how closely you can follow this rule, but the principle still applies.\n",
    "\n",
    "Using `RandomForestClassifier` in scikit-learn\n",
    "- `RandomForestClassifier` (from `sklearn.ensemble`) implements both bagging and split-level feature sampling for you.\n",
    "- The API mirrors `DecisionTreeClassifier`: use `.fit()`, `.predict()`, and `.score()` in the same way.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,     # number of trees\n",
    "    max_features='sqrt', # ~ sqrt(d) features per split\n",
    "    bootstrap=True,      # bootstrapped row samples\n",
    "    n_jobs=-1,           # parallelism\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "rf.fit(x_train, y_train)\n",
    "test_acc = rf.score(x_test, y_test)\n",
    "print(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88b2eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Random Forest — Training, Prediction, and Evaluation Metrics\n",
    "# ------------------------------------------------------------\n",
    "# Checkpoint 1:\n",
    "#   Create a RandomForestClassifier with default parameters,\n",
    "#   retrieve parameters via .get_params(), store as rf_params, and print.\n",
    "#\n",
    "# Checkpoint 2:\n",
    "#   Fit on training data; predict on x_test into y_pred;\n",
    "#   compute accuracy as rf_accuracy and print.\n",
    "#\n",
    "# Checkpoint 3:\n",
    "#   Compute precision, recall, and confusion matrix on test set;\n",
    "#   store as rf_precision, rf_recall, rf_confusion_matrix; print.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Imports\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6075e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d837e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Checkpoint 1 — Instantiate RF and print parameters\n",
    "# ------------------------------------------------------------\n",
    "rf = RandomForestClassifier()     # default parameters\n",
    "rf_params = rf.get_params()       # retrieve parameters dictionary\n",
    "print('Random Forest parameters:')\n",
    "print(rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df909d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:\n",
      "0.9398148148148148\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Checkpoint 2 — Fit, predict, and compute accuracy\n",
    "# ------------------------------------------------------------\n",
    "rf.fit(x_train, y_train)          # train on bootstrapped trees by default\n",
    "y_pred = rf.predict(x_test)       # predicted classes on test set\n",
    "# Either rf.score(...) or accuracy_score(...), both are fine:\n",
    "rf_accuracy = rf.score(x_test, y_test)\n",
    "print('Test set accuracy:')\n",
    "print(rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a904cc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set precision: 0.9435483870967742\n",
      "Test set recall: 0.8602941176470589\n",
      "Test set confusion matrix:\n",
      "[[289   7]\n",
      " [ 19 117]]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Checkpoint 3 — Precision, Recall, Confusion Matrix\n",
    "# ------------------------------------------------------------\n",
    "rf_precision = precision_score(y_test, y_pred)\n",
    "print(f'Test set precision: {rf_precision}')\n",
    "rf_recall = recall_score(y_test, y_pred)\n",
    "print(f'Test set recall: {rf_recall}')\n",
    "rf_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Test set confusion matrix:')\n",
    "print(rf_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d57307",
   "metadata": {},
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "Random forests can be used for **regression** when the target variable is continuous (e.g., a numeric `price`) rather than categorical (e.g., acceptable vs not acceptable). In this setup, each tree predicts a numeric value, and the forest’s prediction is the **average** of the individual trees’ outputs.\n",
    "\n",
    "Key differences vs. classification\n",
    "- **Target type:** continuous values (regression) instead of class labels (classification).\n",
    "- **Aggregation:** average of tree predictions (regression) vs. majority vote or probability averaging (classification).\n",
    "- **Loss/criteria:** trees typically optimize variance reduction (e.g., MSE-based splits) rather than impurity (e.g., Gini/entropy).\n",
    "\n",
    "Using `RandomForestRegressor` (scikit-learn)\n",
    "- Import from `sklearn.ensemble` and use the same workflow as classifiers: `.fit()`, `.predict()`, `.score()`.\n",
    "- The default `.score()` for regressors is the **\\(R^2\\)** (coefficient of determination).\n",
    "\n",
    "Default evaluation: \\(R^2\\)\n",
    "- Measures proportion of variance explained by the model:\n",
    "  \n",
    "  $$\n",
    "  R^2 \\;=\\; 1 - \\frac{\\sum_{i=1}^{n}\\left(y_i - \\hat{y}_i\\right)^2}{\\sum_{i=1}^{n}\\left(y_i - \\bar{y}\\right)^2}\n",
    "  $$\n",
    "  \n",
    "  where \\(y_i\\) are true values, \\(\\hat{y}_i\\) are predictions, and \\(\\bar{y}\\) is the mean of \\(y\\).\n",
    "- Values close to 1 indicate better fit; values \\(\\le 0\\) indicate the model performs no better (or worse) than predicting the mean.\n",
    "\n",
    "Other useful regression metrics\n",
    "- **MSE:** \\( \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\)\n",
    "- **RMSE:** \\( \\text{RMSE} = \\sqrt{\\text{MSE}} \\)\n",
    "- **MAE:** \\( \\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\)\n",
    "\n",
    "Practical notes\n",
    "- The example uses a synthetic continuous `price` to illustrate regression on the same car features.\n",
    "- Typical knobs: `n_estimators`, `max_depth`, `min_samples_leaf`, and `max_features` (often set to `sqrt(d)` or a fraction).\n",
    "- Random forests are robust to nonlinearity and feature interactions; they require minimal preprocessing and handle mixed feature types well (use one-hot encoding for categoricals).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6113241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set R^2: 0.9765\n",
      "Test set R^2:  0.8064\n",
      "Average price (overall): 36,495.70\n",
      "Train set MAE:           1,633.02\n",
      "Test set MAE:            4,678.91\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Random Forest Regressor — Training, Scoring (R^2), and MAE\n",
    "# ------------------------------------------------------------\n",
    "# Checkpoint 1:\n",
    "#   1) Initialize RandomForestRegressor() as rfr\n",
    "#   2) Fit on (x_train, y_train)\n",
    "#   3) Compute default scores (R^2) on train and test:\n",
    "#      - r_squared_train, r_squared_test\n",
    "#   4) Print both R^2 values\n",
    "#\n",
    "# Checkpoint 2:\n",
    "#   1) Compute average price of all cars: avg_price\n",
    "#   2) Compute MAE on train and test:\n",
    "#      - mae_train, mae_test\n",
    "#   3) Print avg_price, mae_train, mae_test\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# ----------------------------\n",
    "# Data Loading & Feature Prep\n",
    "# ----------------------------\n",
    "# Load UCI Car Evaluation dataset (categorical features + acceptability label)\n",
    "df = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data',\n",
    "    names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep']\n",
    ")\n",
    "\n",
    "# (Not used in regression target, but kept for parity with earlier steps)\n",
    "df['accep'] = ~(df['accep'] == 'unacc')\n",
    "\n",
    "# One-hot encode the 6 categorical input features\n",
    "X = pd.get_dummies(df.iloc[:, 0:6], drop_first=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create a synthetic continuous regression target `price`\n",
    "# ------------------------------------------------------------\n",
    "# For illustration only: synthetic prices with trend + noise\n",
    "rng = np.random.default_rng(0)\n",
    "fake_prices = 15000 + 25 * df.index.values + rng.normal(0, 5000, size=df.shape[0])\n",
    "df['price'] = fake_prices\n",
    "y = df['price']\n",
    "\n",
    "# Train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, test_size=0.25\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Checkpoint 1 — Train and Score (R^2)\n",
    "# ------------------------------------------------------------\n",
    "# Step 1.1: Initialize the Random Forest Regressor\n",
    "rfr = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# Step 1.2: Fit the model on training data\n",
    "rfr.fit(x_train, y_train)\n",
    "\n",
    "# Step 1.3: Compute R^2 on train and test sets\n",
    "r_squared_train = rfr.score(x_train, y_train)\n",
    "r_squared_test = rfr.score(x_test, y_test)\n",
    "\n",
    "# Step 1.4: Print R^2 scores\n",
    "print(f\"Train set R^2: {r_squared_train:.4f}\")\n",
    "print(f\"Test set R^2:  {r_squared_test:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Checkpoint 2 — Average Price & Mean Absolute Error (MAE)\n",
    "# ------------------------------------------------------------\n",
    "# Step 2.1: Compute the average price over the whole dataset\n",
    "avg_price = y.mean()\n",
    "\n",
    "# Step 2.2: Predictions for MAE on train and test\n",
    "y_pred_train = rfr.predict(x_train)\n",
    "y_pred_test  = rfr.predict(x_test)\n",
    "\n",
    "# Step 2.3: Compute MAE on train and test sets\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "mae_test  = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "# Step 2.4: Print the mean price and both MAEs\n",
    "print(f\"Average price (overall): {avg_price:,.2f}\")\n",
    "print(f\"Train set MAE:           {mae_train:,.2f}\")\n",
    "print(f\"Test set MAE:            {mae_test:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c514d6",
   "metadata": {},
   "source": [
    "# Final Remarks\n",
    "\n",
    "- **Why it works:** Averaging many de-correlated trees lowers variance and improves stability over a single decision tree.  \n",
    "- **Evaluate with multiple metrics:** Combine \\(R^2\\) with **MAE/RMSE** to gauge both explained variance and error magnitude.  \n",
    "- **Guardrails against overfitting:** Limit tree depth (`max_depth`) or enforce minimum leaf sizes (`min_samples_leaf`), and validate with cross-validation.  \n",
    "- **Feature considerations:** Try `max_features='sqrt'` or a fraction to increase tree diversity; review feature importances for sanity checks, not as causal proof.  \n",
    "- **Practical tuning:** Start with more trees (`n_estimators` 200–500), then adjust `max_depth` and `min_samples_leaf`; use `random_state` for reproducibility and `n_jobs=-1` for speed.  \n",
    "- **Next steps:** Add pipeline steps (scaling if needed for other models), perform grid/random search for hyperparameters, and compare against baselines and simpler regressors.\n",
    "\n",
    "Bottom line: `RandomForestRegressor` is a strong, low-maintenance baseline for nonlinear tabular regression—robust, interpretable enough to debug, and a solid benchmark for more advanced methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b22749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
